{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f503d3",
   "metadata": {},
   "source": [
    "# ClasificaciÃ³n de grafos con redout Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5b73626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: MUTAG(188):\n",
      "====================\n",
      "Number of graphs: 188\n",
      "Number of features: 7\n",
      "Number of classes: 2\n",
      "\n",
      "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 17\n",
      "Number of edges: 38\n",
      "Average node degree: 2.24\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/ahmedbegga/Desktop/TFG-Ahmed/SetXAI/')\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import *\n",
    "from src.fspool import FSPool\n",
    "from torch_scatter import scatter\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,DenseGraphConv, dense_mincut_pool\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool,global_sort_pool\n",
    "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "#torch.manual_seed(12345)\n",
    "#print(f'Number of training graphs: {len(train_dataset)}')\n",
    "#print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d75c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 766], x=[350, 7], edge_attr=[766, 4], y=[20], batch=[350], ptr=[21])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 774], x=[350, 7], edge_attr=[774, 4], y=[20], batch=[350], ptr=[21])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 792], x=[360, 7], edge_attr=[792, 4], y=[20], batch=[360], ptr=[21])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 756], x=[346, 7], edge_attr=[756, 4], y=[20], batch=[346], ptr=[21])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 768], x=[351, 7], edge_attr=[768, 4], y=[20], batch=[351], ptr=[21])\n",
      "\n",
      "Step 6:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 866], x=[387, 7], edge_attr=[866, 4], y=[20], batch=[387], ptr=[21])\n",
      "\n",
      "Step 7:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 800], x=[359, 7], edge_attr=[800, 4], y=[20], batch=[359], ptr=[21])\n",
      "\n",
      "Step 8:\n",
      "=======\n",
      "Number of graphs in the current batch: 10\n",
      "DataBatch(edge_index=[2, 408], x=[181, 7], edge_attr=[408, 4], y=[10], batch=[181], ptr=[11])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(dataset[:150], batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(dataset[150:], batch_size=20, shuffle=False)\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c8e53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=32):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # GCN Layer - MLP - Dense GCN Layer\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        num_of_centers =  16 # k\n",
    "        # The degree of the node belonging to any of the centers\n",
    "        self.pool1 = Linear(hidden_channels, num_of_centers) \n",
    "        self.conv2 = DenseGraphConv(hidden_channels, hidden_channels)\n",
    "        # MLPs towards out \n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "        self.weight = torch.nn.Parameter(torch.zeros(16,32))\n",
    "        torch.nn.init.normal_(self.weight)\n",
    "        # Input: Batch of 20 graphs, each node F=3 features \n",
    "        #        N1 + N2 + ... + N2 = 661\n",
    "        # TSNE here?\n",
    "        self.salidaConv = torch.zeros(0)\n",
    "    def forward(self, x, edge_index, batch):    # x torch.Size([661, 3]),  data.batch  torch.Size([661])  \n",
    "        # CONV1: Expand features from F=3 to F' = 32\n",
    "        x = F.relu(self.conv1(x, edge_index))   # x torch.Size([661, 32]) \n",
    "        \n",
    "        # Make all x_i of size N=MAX(N1,...,N20), e.g. N=40: \n",
    "        x, mask = to_dense_batch(x, batch)      # x torch.Size([20, N, 32]) ; mask torch.Size([20, N]) batch_size=20\n",
    "        #print(\"x size\", x.size())\n",
    "        \n",
    "        # Make all adjacencies of size NxN \n",
    "        adj = to_dense_adj(edge_index, batch)   # adj torch.Size([20, N, N])\n",
    "        #print(\"adj_size\", adj.size())\n",
    "        \n",
    "        # MLP of k=16 outputs s\n",
    "        #print(\"adj_size\", adj.size())\n",
    "        s = self.pool1(x) # s torch.Size([20, N, k])\n",
    "        #print(\"s_size\", s.size())\n",
    "        self.salidaConv = x\n",
    "        # MINCUT_POOL\n",
    "        # Call to dense_cut_mincut_pool to get coarsened x, adj and the losses: k=16\n",
    "        x, adj, mincut_loss, ortho_loss = dense_mincut_pool(x, adj, s, mask) # x torch.Size([20, 16, 32]),  adj torch.Size([20, 16, 16])\n",
    "        #print(\"Coarsened x and adj sizes:\", x.size(), adj.size())\n",
    "\n",
    "        # CONV2: Now on coarsened x and adj: \n",
    "        x = self.conv2(x, adj) #x torch.Size([20, 16, 32])\n",
    "        #print(\"x_2 size\", x.size())\n",
    "\n",
    "        # Readout for each of the 20 graphs\n",
    "        #x = x.mean(dim=1) # x torch.Size([20, 32])\n",
    "        x = x.sum(dim=1) # x torch.Size([20, 32])\n",
    "        #x, _ = x.sort(dim=1)\n",
    "        #x = torch.einsum('nlc, lc -> nc', x, self.weight)\n",
    "        #print(x.shape)\n",
    "        #x =  x * self.weight\n",
    "        #x = x.max(dim=1)[0]\n",
    "        #x = x.max(dim=1)[0]\n",
    "        #print(\"mean x_2 size\", x.size())\n",
    "        # Final MLP for graph classification: hidden channels = 32\n",
    "        x = F.relu(self.lin1(x)) # x torch.Size([20, 32])\n",
    "        #print(\"final x1 size\", x.size())\n",
    "        x = self.lin2(x) #x torch.Size([20, 2])\n",
    "        #print(\"final x2 size\", x.size())\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b0cefc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.626, Train Acc: 0.693, Test Loss: 0.700, Test Acc: 0.553\n",
      "Epoch: 002, Train Loss: 0.553, Train Acc: 0.693, Test Loss: 0.673, Test Acc: 0.553\n",
      "Epoch: 003, Train Loss: 0.512, Train Acc: 0.693, Test Loss: 0.589, Test Acc: 0.553\n",
      "Epoch: 004, Train Loss: 0.500, Train Acc: 0.753, Test Loss: 0.559, Test Acc: 0.658\n",
      "Epoch: 005, Train Loss: 0.471, Train Acc: 0.740, Test Loss: 0.547, Test Acc: 0.658\n",
      "Epoch: 006, Train Loss: 0.449, Train Acc: 0.767, Test Loss: 0.524, Test Acc: 0.711\n",
      "Epoch: 007, Train Loss: 0.432, Train Acc: 0.813, Test Loss: 0.488, Test Acc: 0.789\n",
      "Epoch: 008, Train Loss: 0.418, Train Acc: 0.847, Test Loss: 0.463, Test Acc: 0.789\n",
      "Epoch: 009, Train Loss: 0.404, Train Acc: 0.800, Test Loss: 0.475, Test Acc: 0.763\n",
      "Epoch: 010, Train Loss: 0.390, Train Acc: 0.860, Test Loss: 0.434, Test Acc: 0.868\n",
      "Epoch: 011, Train Loss: 0.375, Train Acc: 0.853, Test Loss: 0.444, Test Acc: 0.789\n",
      "Epoch: 012, Train Loss: 0.373, Train Acc: 0.853, Test Loss: 0.417, Test Acc: 0.842\n",
      "Epoch: 013, Train Loss: 0.365, Train Acc: 0.853, Test Loss: 0.411, Test Acc: 0.842\n",
      "Epoch: 014, Train Loss: 0.366, Train Acc: 0.853, Test Loss: 0.403, Test Acc: 0.842\n",
      "Epoch: 015, Train Loss: 0.360, Train Acc: 0.853, Test Loss: 0.406, Test Acc: 0.842\n",
      "Epoch: 016, Train Loss: 0.358, Train Acc: 0.853, Test Loss: 0.398, Test Acc: 0.816\n",
      "Epoch: 017, Train Loss: 0.357, Train Acc: 0.853, Test Loss: 0.402, Test Acc: 0.842\n",
      "Epoch: 018, Train Loss: 0.358, Train Acc: 0.860, Test Loss: 0.397, Test Acc: 0.816\n",
      "Epoch: 019, Train Loss: 0.362, Train Acc: 0.860, Test Loss: 0.394, Test Acc: 0.842\n",
      "Epoch: 020, Train Loss: 0.364, Train Acc: 0.853, Test Loss: 0.399, Test Acc: 0.816\n",
      "Epoch: 021, Train Loss: 0.361, Train Acc: 0.853, Test Loss: 0.402, Test Acc: 0.816\n",
      "Epoch: 022, Train Loss: 0.367, Train Acc: 0.860, Test Loss: 0.398, Test Acc: 0.842\n",
      "Epoch: 023, Train Loss: 0.351, Train Acc: 0.847, Test Loss: 0.408, Test Acc: 0.816\n",
      "Epoch: 024, Train Loss: 0.359, Train Acc: 0.860, Test Loss: 0.392, Test Acc: 0.842\n",
      "Epoch: 025, Train Loss: 0.360, Train Acc: 0.860, Test Loss: 0.392, Test Acc: 0.842\n",
      "Epoch: 026, Train Loss: 0.365, Train Acc: 0.847, Test Loss: 0.410, Test Acc: 0.816\n",
      "Epoch: 027, Train Loss: 0.350, Train Acc: 0.860, Test Loss: 0.391, Test Acc: 0.842\n",
      "Epoch: 028, Train Loss: 0.362, Train Acc: 0.860, Test Loss: 0.391, Test Acc: 0.842\n",
      "Epoch: 029, Train Loss: 0.353, Train Acc: 0.860, Test Loss: 0.396, Test Acc: 0.842\n",
      "Epoch: 030, Train Loss: 0.351, Train Acc: 0.853, Test Loss: 0.398, Test Acc: 0.816\n",
      "Epoch: 031, Train Loss: 0.357, Train Acc: 0.853, Test Loss: 0.396, Test Acc: 0.816\n",
      "Epoch: 032, Train Loss: 0.351, Train Acc: 0.860, Test Loss: 0.390, Test Acc: 0.842\n",
      "Epoch: 033, Train Loss: 0.354, Train Acc: 0.860, Test Loss: 0.390, Test Acc: 0.842\n",
      "Epoch: 034, Train Loss: 0.353, Train Acc: 0.853, Test Loss: 0.400, Test Acc: 0.816\n",
      "Epoch: 035, Train Loss: 0.361, Train Acc: 0.860, Test Loss: 0.394, Test Acc: 0.842\n",
      "Epoch: 036, Train Loss: 0.357, Train Acc: 0.853, Test Loss: 0.399, Test Acc: 0.816\n",
      "Epoch: 037, Train Loss: 0.353, Train Acc: 0.860, Test Loss: 0.393, Test Acc: 0.842\n",
      "Epoch: 038, Train Loss: 0.353, Train Acc: 0.860, Test Loss: 0.397, Test Acc: 0.842\n",
      "Epoch: 039, Train Loss: 0.352, Train Acc: 0.860, Test Loss: 0.391, Test Acc: 0.842\n",
      "Epoch: 040, Train Loss: 0.351, Train Acc: 0.860, Test Loss: 0.394, Test Acc: 0.842\n",
      "Epoch: 041, Train Loss: 0.354, Train Acc: 0.860, Test Loss: 0.392, Test Acc: 0.842\n",
      "Epoch: 042, Train Loss: 0.359, Train Acc: 0.860, Test Loss: 0.394, Test Acc: 0.842\n",
      "Epoch: 043, Train Loss: 0.352, Train Acc: 0.847, Test Loss: 0.411, Test Acc: 0.789\n",
      "Epoch: 044, Train Loss: 0.353, Train Acc: 0.860, Test Loss: 0.396, Test Acc: 0.842\n",
      "Epoch: 045, Train Loss: 0.356, Train Acc: 0.860, Test Loss: 0.396, Test Acc: 0.842\n",
      "Epoch: 046, Train Loss: 0.355, Train Acc: 0.860, Test Loss: 0.399, Test Acc: 0.842\n",
      "Epoch: 047, Train Loss: 0.351, Train Acc: 0.853, Test Loss: 0.405, Test Acc: 0.816\n",
      "Epoch: 048, Train Loss: 0.352, Train Acc: 0.860, Test Loss: 0.397, Test Acc: 0.842\n",
      "Epoch: 049, Train Loss: 0.356, Train Acc: 0.860, Test Loss: 0.396, Test Acc: 0.842\n",
      "RUN:0,Test Acc:0.8421\n",
      "Epoch: 001, Train Loss: 0.549, Train Acc: 0.673, Test Loss: 0.591, Test Acc: 0.658\n",
      "Epoch: 002, Train Loss: 0.474, Train Acc: 0.767, Test Loss: 0.583, Test Acc: 0.632\n",
      "Epoch: 003, Train Loss: 0.435, Train Acc: 0.840, Test Loss: 0.547, Test Acc: 0.711\n",
      "Epoch: 004, Train Loss: 0.405, Train Acc: 0.867, Test Loss: 0.551, Test Acc: 0.711\n",
      "Epoch: 005, Train Loss: 0.380, Train Acc: 0.860, Test Loss: 0.549, Test Acc: 0.711\n",
      "Epoch: 006, Train Loss: 0.366, Train Acc: 0.860, Test Loss: 0.573, Test Acc: 0.711\n",
      "Epoch: 007, Train Loss: 0.357, Train Acc: 0.860, Test Loss: 0.588, Test Acc: 0.711\n",
      "Epoch: 008, Train Loss: 0.347, Train Acc: 0.867, Test Loss: 0.550, Test Acc: 0.711\n",
      "Epoch: 009, Train Loss: 0.340, Train Acc: 0.860, Test Loss: 0.598, Test Acc: 0.711\n",
      "Epoch: 010, Train Loss: 0.339, Train Acc: 0.873, Test Loss: 0.553, Test Acc: 0.737\n",
      "Epoch: 011, Train Loss: 0.341, Train Acc: 0.867, Test Loss: 0.578, Test Acc: 0.711\n",
      "Epoch: 012, Train Loss: 0.329, Train Acc: 0.887, Test Loss: 0.528, Test Acc: 0.737\n",
      "Epoch: 013, Train Loss: 0.323, Train Acc: 0.860, Test Loss: 0.599, Test Acc: 0.711\n",
      "Epoch: 014, Train Loss: 0.322, Train Acc: 0.887, Test Loss: 0.571, Test Acc: 0.737\n",
      "Epoch: 015, Train Loss: 0.335, Train Acc: 0.867, Test Loss: 0.601, Test Acc: 0.711\n",
      "Epoch: 016, Train Loss: 0.321, Train Acc: 0.887, Test Loss: 0.559, Test Acc: 0.763\n",
      "Epoch: 017, Train Loss: 0.334, Train Acc: 0.887, Test Loss: 0.563, Test Acc: 0.763\n",
      "Epoch: 018, Train Loss: 0.318, Train Acc: 0.880, Test Loss: 0.533, Test Acc: 0.763\n",
      "Epoch: 019, Train Loss: 0.323, Train Acc: 0.887, Test Loss: 0.582, Test Acc: 0.763\n",
      "Epoch: 020, Train Loss: 0.331, Train Acc: 0.887, Test Loss: 0.580, Test Acc: 0.763\n",
      "Epoch: 021, Train Loss: 0.317, Train Acc: 0.880, Test Loss: 0.551, Test Acc: 0.763\n",
      "Epoch: 022, Train Loss: 0.329, Train Acc: 0.887, Test Loss: 0.585, Test Acc: 0.763\n",
      "Epoch: 023, Train Loss: 0.319, Train Acc: 0.880, Test Loss: 0.563, Test Acc: 0.763\n",
      "Epoch: 024, Train Loss: 0.334, Train Acc: 0.887, Test Loss: 0.582, Test Acc: 0.763\n",
      "Epoch: 025, Train Loss: 0.322, Train Acc: 0.880, Test Loss: 0.577, Test Acc: 0.763\n",
      "Epoch: 026, Train Loss: 0.320, Train Acc: 0.880, Test Loss: 0.544, Test Acc: 0.763\n",
      "Epoch: 027, Train Loss: 0.316, Train Acc: 0.873, Test Loss: 0.621, Test Acc: 0.737\n",
      "Epoch: 028, Train Loss: 0.320, Train Acc: 0.880, Test Loss: 0.568, Test Acc: 0.763\n",
      "Epoch: 029, Train Loss: 0.316, Train Acc: 0.880, Test Loss: 0.568, Test Acc: 0.763\n",
      "Epoch: 030, Train Loss: 0.314, Train Acc: 0.880, Test Loss: 0.594, Test Acc: 0.763\n",
      "Epoch: 031, Train Loss: 0.313, Train Acc: 0.880, Test Loss: 0.556, Test Acc: 0.763\n",
      "Epoch: 032, Train Loss: 0.344, Train Acc: 0.880, Test Loss: 0.575, Test Acc: 0.763\n",
      "Epoch: 033, Train Loss: 0.320, Train Acc: 0.893, Test Loss: 0.548, Test Acc: 0.763\n",
      "Epoch: 034, Train Loss: 0.321, Train Acc: 0.880, Test Loss: 0.590, Test Acc: 0.763\n",
      "Epoch: 035, Train Loss: 0.312, Train Acc: 0.880, Test Loss: 0.561, Test Acc: 0.763\n",
      "Epoch: 036, Train Loss: 0.315, Train Acc: 0.880, Test Loss: 0.581, Test Acc: 0.763\n",
      "Epoch: 037, Train Loss: 0.313, Train Acc: 0.880, Test Loss: 0.579, Test Acc: 0.763\n",
      "Epoch: 038, Train Loss: 0.314, Train Acc: 0.880, Test Loss: 0.579, Test Acc: 0.763\n",
      "Epoch: 039, Train Loss: 0.317, Train Acc: 0.880, Test Loss: 0.580, Test Acc: 0.763\n",
      "Epoch: 040, Train Loss: 0.325, Train Acc: 0.880, Test Loss: 0.596, Test Acc: 0.763\n",
      "Epoch: 041, Train Loss: 0.326, Train Acc: 0.880, Test Loss: 0.577, Test Acc: 0.763\n",
      "Epoch: 042, Train Loss: 0.313, Train Acc: 0.873, Test Loss: 0.632, Test Acc: 0.737\n",
      "Epoch: 043, Train Loss: 0.315, Train Acc: 0.880, Test Loss: 0.573, Test Acc: 0.763\n",
      "Epoch: 044, Train Loss: 0.320, Train Acc: 0.880, Test Loss: 0.573, Test Acc: 0.763\n",
      "Epoch: 045, Train Loss: 0.321, Train Acc: 0.880, Test Loss: 0.602, Test Acc: 0.763\n",
      "Epoch: 046, Train Loss: 0.317, Train Acc: 0.880, Test Loss: 0.567, Test Acc: 0.763\n",
      "Epoch: 047, Train Loss: 0.310, Train Acc: 0.873, Test Loss: 0.636, Test Acc: 0.737\n",
      "Epoch: 048, Train Loss: 0.319, Train Acc: 0.880, Test Loss: 0.578, Test Acc: 0.763\n",
      "Epoch: 049, Train Loss: 0.315, Train Acc: 0.880, Test Loss: 0.591, Test Acc: 0.763\n",
      "RUN:1,Test Acc:0.7632\n",
      "Epoch: 001, Train Loss: 0.717, Train Acc: 0.647, Test Loss: 0.420, Test Acc: 0.737\n",
      "Epoch: 002, Train Loss: 0.597, Train Acc: 0.647, Test Loss: 0.446, Test Acc: 0.737\n",
      "Epoch: 003, Train Loss: 0.564, Train Acc: 0.647, Test Loss: 0.414, Test Acc: 0.737\n",
      "Epoch: 004, Train Loss: 0.531, Train Acc: 0.700, Test Loss: 0.370, Test Acc: 0.763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Loss: 0.503, Train Acc: 0.780, Test Loss: 0.342, Test Acc: 0.763\n",
      "Epoch: 006, Train Loss: 0.475, Train Acc: 0.800, Test Loss: 0.318, Test Acc: 0.868\n",
      "Epoch: 007, Train Loss: 0.456, Train Acc: 0.813, Test Loss: 0.297, Test Acc: 0.868\n",
      "Epoch: 008, Train Loss: 0.441, Train Acc: 0.827, Test Loss: 0.267, Test Acc: 0.868\n",
      "Epoch: 009, Train Loss: 0.423, Train Acc: 0.820, Test Loss: 0.255, Test Acc: 0.868\n",
      "Epoch: 010, Train Loss: 0.417, Train Acc: 0.827, Test Loss: 0.246, Test Acc: 0.868\n",
      "Epoch: 011, Train Loss: 0.420, Train Acc: 0.827, Test Loss: 0.236, Test Acc: 0.868\n",
      "Epoch: 012, Train Loss: 0.415, Train Acc: 0.827, Test Loss: 0.230, Test Acc: 0.868\n",
      "Epoch: 013, Train Loss: 0.390, Train Acc: 0.860, Test Loss: 0.238, Test Acc: 0.868\n",
      "Epoch: 014, Train Loss: 0.392, Train Acc: 0.820, Test Loss: 0.220, Test Acc: 0.895\n",
      "Epoch: 015, Train Loss: 0.391, Train Acc: 0.847, Test Loss: 0.217, Test Acc: 0.895\n",
      "Epoch: 016, Train Loss: 0.416, Train Acc: 0.853, Test Loss: 0.215, Test Acc: 0.895\n",
      "Epoch: 017, Train Loss: 0.407, Train Acc: 0.847, Test Loss: 0.216, Test Acc: 0.895\n",
      "Epoch: 018, Train Loss: 0.383, Train Acc: 0.860, Test Loss: 0.218, Test Acc: 0.868\n",
      "Epoch: 019, Train Loss: 0.388, Train Acc: 0.847, Test Loss: 0.212, Test Acc: 0.895\n",
      "Epoch: 020, Train Loss: 0.387, Train Acc: 0.847, Test Loss: 0.210, Test Acc: 0.895\n",
      "Epoch: 021, Train Loss: 0.386, Train Acc: 0.847, Test Loss: 0.209, Test Acc: 0.895\n",
      "Epoch: 022, Train Loss: 0.387, Train Acc: 0.853, Test Loss: 0.211, Test Acc: 0.895\n",
      "Epoch: 023, Train Loss: 0.385, Train Acc: 0.860, Test Loss: 0.210, Test Acc: 0.868\n",
      "Epoch: 024, Train Loss: 0.396, Train Acc: 0.847, Test Loss: 0.207, Test Acc: 0.895\n",
      "Epoch: 025, Train Loss: 0.378, Train Acc: 0.860, Test Loss: 0.206, Test Acc: 0.868\n",
      "Epoch: 026, Train Loss: 0.378, Train Acc: 0.847, Test Loss: 0.206, Test Acc: 0.895\n",
      "Epoch: 027, Train Loss: 0.398, Train Acc: 0.847, Test Loss: 0.202, Test Acc: 0.895\n",
      "Epoch: 028, Train Loss: 0.373, Train Acc: 0.853, Test Loss: 0.207, Test Acc: 0.868\n",
      "Epoch: 029, Train Loss: 0.382, Train Acc: 0.847, Test Loss: 0.205, Test Acc: 0.895\n",
      "Epoch: 030, Train Loss: 0.388, Train Acc: 0.847, Test Loss: 0.203, Test Acc: 0.895\n",
      "Epoch: 031, Train Loss: 0.379, Train Acc: 0.860, Test Loss: 0.202, Test Acc: 0.868\n",
      "Epoch: 032, Train Loss: 0.389, Train Acc: 0.847, Test Loss: 0.208, Test Acc: 0.895\n",
      "Epoch: 033, Train Loss: 0.380, Train Acc: 0.860, Test Loss: 0.198, Test Acc: 0.895\n",
      "Epoch: 034, Train Loss: 0.375, Train Acc: 0.847, Test Loss: 0.212, Test Acc: 0.895\n",
      "Epoch: 035, Train Loss: 0.392, Train Acc: 0.860, Test Loss: 0.200, Test Acc: 0.895\n",
      "Epoch: 036, Train Loss: 0.375, Train Acc: 0.847, Test Loss: 0.207, Test Acc: 0.895\n",
      "Epoch: 037, Train Loss: 0.384, Train Acc: 0.860, Test Loss: 0.201, Test Acc: 0.895\n",
      "Epoch: 038, Train Loss: 0.398, Train Acc: 0.847, Test Loss: 0.210, Test Acc: 0.895\n",
      "Epoch: 039, Train Loss: 0.390, Train Acc: 0.827, Test Loss: 0.213, Test Acc: 0.842\n",
      "Epoch: 040, Train Loss: 0.378, Train Acc: 0.853, Test Loss: 0.218, Test Acc: 0.895\n",
      "Epoch: 041, Train Loss: 0.387, Train Acc: 0.847, Test Loss: 0.204, Test Acc: 0.895\n",
      "Epoch: 042, Train Loss: 0.392, Train Acc: 0.860, Test Loss: 0.202, Test Acc: 0.868\n",
      "Epoch: 043, Train Loss: 0.366, Train Acc: 0.853, Test Loss: 0.221, Test Acc: 0.895\n",
      "Epoch: 044, Train Loss: 0.391, Train Acc: 0.860, Test Loss: 0.201, Test Acc: 0.895\n",
      "Epoch: 045, Train Loss: 0.376, Train Acc: 0.860, Test Loss: 0.198, Test Acc: 0.895\n",
      "Epoch: 046, Train Loss: 0.371, Train Acc: 0.847, Test Loss: 0.206, Test Acc: 0.895\n",
      "Epoch: 047, Train Loss: 0.377, Train Acc: 0.847, Test Loss: 0.207, Test Acc: 0.895\n",
      "Epoch: 048, Train Loss: 0.367, Train Acc: 0.853, Test Loss: 0.198, Test Acc: 0.868\n",
      "Epoch: 049, Train Loss: 0.388, Train Acc: 0.847, Test Loss: 0.203, Test Acc: 0.895\n",
      "RUN:2,Test Acc:0.8947\n",
      "Epoch: 001, Train Loss: 0.631, Train Acc: 0.640, Test Loss: 0.526, Test Acc: 0.763\n",
      "Epoch: 002, Train Loss: 0.551, Train Acc: 0.640, Test Loss: 0.511, Test Acc: 0.763\n",
      "Epoch: 003, Train Loss: 0.524, Train Acc: 0.787, Test Loss: 0.512, Test Acc: 0.816\n",
      "Epoch: 004, Train Loss: 0.495, Train Acc: 0.740, Test Loss: 0.483, Test Acc: 0.789\n",
      "Epoch: 005, Train Loss: 0.468, Train Acc: 0.833, Test Loss: 0.477, Test Acc: 0.842\n",
      "Epoch: 006, Train Loss: 0.434, Train Acc: 0.833, Test Loss: 0.468, Test Acc: 0.842\n",
      "Epoch: 007, Train Loss: 0.417, Train Acc: 0.840, Test Loss: 0.473, Test Acc: 0.816\n",
      "Epoch: 008, Train Loss: 0.394, Train Acc: 0.840, Test Loss: 0.480, Test Acc: 0.816\n",
      "Epoch: 009, Train Loss: 0.375, Train Acc: 0.833, Test Loss: 0.490, Test Acc: 0.816\n",
      "Epoch: 010, Train Loss: 0.367, Train Acc: 0.860, Test Loss: 0.513, Test Acc: 0.816\n",
      "Epoch: 011, Train Loss: 0.353, Train Acc: 0.840, Test Loss: 0.527, Test Acc: 0.816\n",
      "Epoch: 012, Train Loss: 0.354, Train Acc: 0.867, Test Loss: 0.538, Test Acc: 0.816\n",
      "Epoch: 013, Train Loss: 0.351, Train Acc: 0.873, Test Loss: 0.555, Test Acc: 0.816\n",
      "Epoch: 014, Train Loss: 0.342, Train Acc: 0.867, Test Loss: 0.573, Test Acc: 0.816\n",
      "Epoch: 015, Train Loss: 0.335, Train Acc: 0.873, Test Loss: 0.587, Test Acc: 0.816\n",
      "Epoch: 016, Train Loss: 0.334, Train Acc: 0.867, Test Loss: 0.613, Test Acc: 0.816\n",
      "Epoch: 017, Train Loss: 0.337, Train Acc: 0.880, Test Loss: 0.613, Test Acc: 0.816\n",
      "Epoch: 018, Train Loss: 0.329, Train Acc: 0.873, Test Loss: 0.625, Test Acc: 0.816\n",
      "Epoch: 019, Train Loss: 0.329, Train Acc: 0.867, Test Loss: 0.630, Test Acc: 0.789\n",
      "Epoch: 020, Train Loss: 0.328, Train Acc: 0.867, Test Loss: 0.628, Test Acc: 0.816\n",
      "Epoch: 021, Train Loss: 0.338, Train Acc: 0.867, Test Loss: 0.634, Test Acc: 0.816\n",
      "Epoch: 022, Train Loss: 0.326, Train Acc: 0.867, Test Loss: 0.638, Test Acc: 0.816\n",
      "Epoch: 023, Train Loss: 0.321, Train Acc: 0.867, Test Loss: 0.654, Test Acc: 0.789\n",
      "Epoch: 024, Train Loss: 0.332, Train Acc: 0.867, Test Loss: 0.648, Test Acc: 0.816\n",
      "Epoch: 025, Train Loss: 0.345, Train Acc: 0.880, Test Loss: 0.653, Test Acc: 0.816\n",
      "Epoch: 026, Train Loss: 0.327, Train Acc: 0.867, Test Loss: 0.653, Test Acc: 0.816\n",
      "Epoch: 027, Train Loss: 0.327, Train Acc: 0.880, Test Loss: 0.655, Test Acc: 0.816\n",
      "Epoch: 028, Train Loss: 0.319, Train Acc: 0.867, Test Loss: 0.673, Test Acc: 0.789\n",
      "Epoch: 029, Train Loss: 0.322, Train Acc: 0.867, Test Loss: 0.664, Test Acc: 0.816\n",
      "Epoch: 030, Train Loss: 0.324, Train Acc: 0.873, Test Loss: 0.684, Test Acc: 0.763\n",
      "Epoch: 031, Train Loss: 0.325, Train Acc: 0.880, Test Loss: 0.672, Test Acc: 0.816\n",
      "Epoch: 032, Train Loss: 0.321, Train Acc: 0.880, Test Loss: 0.681, Test Acc: 0.816\n",
      "Epoch: 033, Train Loss: 0.322, Train Acc: 0.867, Test Loss: 0.666, Test Acc: 0.816\n",
      "Epoch: 034, Train Loss: 0.341, Train Acc: 0.867, Test Loss: 0.647, Test Acc: 0.816\n",
      "Epoch: 035, Train Loss: 0.337, Train Acc: 0.880, Test Loss: 0.638, Test Acc: 0.816\n",
      "Epoch: 036, Train Loss: 0.338, Train Acc: 0.867, Test Loss: 0.656, Test Acc: 0.789\n",
      "Epoch: 037, Train Loss: 0.312, Train Acc: 0.873, Test Loss: 0.642, Test Acc: 0.816\n",
      "Epoch: 038, Train Loss: 0.334, Train Acc: 0.867, Test Loss: 0.671, Test Acc: 0.789\n",
      "Epoch: 039, Train Loss: 0.326, Train Acc: 0.880, Test Loss: 0.666, Test Acc: 0.816\n",
      "Epoch: 040, Train Loss: 0.322, Train Acc: 0.880, Test Loss: 0.666, Test Acc: 0.816\n",
      "Epoch: 041, Train Loss: 0.321, Train Acc: 0.880, Test Loss: 0.667, Test Acc: 0.816\n",
      "Epoch: 042, Train Loss: 0.319, Train Acc: 0.880, Test Loss: 0.678, Test Acc: 0.816\n",
      "Epoch: 043, Train Loss: 0.335, Train Acc: 0.880, Test Loss: 0.674, Test Acc: 0.816\n",
      "Epoch: 044, Train Loss: 0.318, Train Acc: 0.867, Test Loss: 0.667, Test Acc: 0.816\n",
      "Epoch: 045, Train Loss: 0.316, Train Acc: 0.880, Test Loss: 0.686, Test Acc: 0.816\n",
      "Epoch: 046, Train Loss: 0.319, Train Acc: 0.880, Test Loss: 0.698, Test Acc: 0.789\n",
      "Epoch: 047, Train Loss: 0.316, Train Acc: 0.867, Test Loss: 0.666, Test Acc: 0.816\n",
      "Epoch: 048, Train Loss: 0.333, Train Acc: 0.867, Test Loss: 0.696, Test Acc: 0.789\n",
      "Epoch: 049, Train Loss: 0.309, Train Acc: 0.873, Test Loss: 0.662, Test Acc: 0.816\n",
      "RUN:3,Test Acc:0.8158\n",
      "Epoch: 001, Train Loss: 0.617, Train Acc: 0.667, Test Loss: 0.483, Test Acc: 0.658\n",
      "Epoch: 002, Train Loss: 0.539, Train Acc: 0.667, Test Loss: 0.453, Test Acc: 0.658\n",
      "Epoch: 003, Train Loss: 0.511, Train Acc: 0.740, Test Loss: 0.443, Test Acc: 0.789\n",
      "Epoch: 004, Train Loss: 0.480, Train Acc: 0.720, Test Loss: 0.403, Test Acc: 0.737\n",
      "Epoch: 005, Train Loss: 0.458, Train Acc: 0.807, Test Loss: 0.387, Test Acc: 0.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train Loss: 0.435, Train Acc: 0.833, Test Loss: 0.363, Test Acc: 0.842\n",
      "Epoch: 007, Train Loss: 0.415, Train Acc: 0.833, Test Loss: 0.332, Test Acc: 0.842\n",
      "Epoch: 008, Train Loss: 0.423, Train Acc: 0.833, Test Loss: 0.315, Test Acc: 0.842\n",
      "Epoch: 009, Train Loss: 0.396, Train Acc: 0.853, Test Loss: 0.323, Test Acc: 0.816\n",
      "Epoch: 010, Train Loss: 0.390, Train Acc: 0.833, Test Loss: 0.286, Test Acc: 0.789\n",
      "Epoch: 011, Train Loss: 0.373, Train Acc: 0.853, Test Loss: 0.296, Test Acc: 0.816\n",
      "Epoch: 012, Train Loss: 0.380, Train Acc: 0.840, Test Loss: 0.270, Test Acc: 0.816\n",
      "Epoch: 013, Train Loss: 0.367, Train Acc: 0.860, Test Loss: 0.266, Test Acc: 0.816\n",
      "Epoch: 014, Train Loss: 0.362, Train Acc: 0.853, Test Loss: 0.249, Test Acc: 0.816\n",
      "Epoch: 015, Train Loss: 0.359, Train Acc: 0.860, Test Loss: 0.244, Test Acc: 0.816\n",
      "Epoch: 016, Train Loss: 0.357, Train Acc: 0.867, Test Loss: 0.241, Test Acc: 0.816\n",
      "Epoch: 017, Train Loss: 0.359, Train Acc: 0.867, Test Loss: 0.236, Test Acc: 0.816\n",
      "Epoch: 018, Train Loss: 0.373, Train Acc: 0.867, Test Loss: 0.235, Test Acc: 0.816\n",
      "Epoch: 019, Train Loss: 0.360, Train Acc: 0.847, Test Loss: 0.227, Test Acc: 0.816\n",
      "Epoch: 020, Train Loss: 0.358, Train Acc: 0.860, Test Loss: 0.245, Test Acc: 0.868\n",
      "Epoch: 021, Train Loss: 0.351, Train Acc: 0.867, Test Loss: 0.226, Test Acc: 0.816\n",
      "Epoch: 022, Train Loss: 0.358, Train Acc: 0.867, Test Loss: 0.225, Test Acc: 0.842\n",
      "Epoch: 023, Train Loss: 0.358, Train Acc: 0.867, Test Loss: 0.224, Test Acc: 0.842\n",
      "Epoch: 024, Train Loss: 0.386, Train Acc: 0.867, Test Loss: 0.239, Test Acc: 0.868\n",
      "Epoch: 025, Train Loss: 0.376, Train Acc: 0.840, Test Loss: 0.232, Test Acc: 0.789\n",
      "Epoch: 026, Train Loss: 0.354, Train Acc: 0.860, Test Loss: 0.241, Test Acc: 0.868\n",
      "Epoch: 027, Train Loss: 0.351, Train Acc: 0.867, Test Loss: 0.221, Test Acc: 0.842\n",
      "Epoch: 028, Train Loss: 0.353, Train Acc: 0.867, Test Loss: 0.219, Test Acc: 0.842\n",
      "Epoch: 029, Train Loss: 0.349, Train Acc: 0.860, Test Loss: 0.222, Test Acc: 0.842\n",
      "Epoch: 030, Train Loss: 0.347, Train Acc: 0.867, Test Loss: 0.211, Test Acc: 0.842\n",
      "Epoch: 031, Train Loss: 0.341, Train Acc: 0.867, Test Loss: 0.222, Test Acc: 0.868\n",
      "Epoch: 032, Train Loss: 0.348, Train Acc: 0.867, Test Loss: 0.209, Test Acc: 0.842\n",
      "Epoch: 033, Train Loss: 0.343, Train Acc: 0.867, Test Loss: 0.209, Test Acc: 0.842\n",
      "Epoch: 034, Train Loss: 0.346, Train Acc: 0.867, Test Loss: 0.205, Test Acc: 0.842\n",
      "Epoch: 035, Train Loss: 0.347, Train Acc: 0.867, Test Loss: 0.203, Test Acc: 0.842\n",
      "Epoch: 036, Train Loss: 0.340, Train Acc: 0.867, Test Loss: 0.201, Test Acc: 0.842\n",
      "Epoch: 037, Train Loss: 0.338, Train Acc: 0.867, Test Loss: 0.200, Test Acc: 0.842\n",
      "Epoch: 038, Train Loss: 0.344, Train Acc: 0.867, Test Loss: 0.199, Test Acc: 0.868\n",
      "Epoch: 039, Train Loss: 0.336, Train Acc: 0.867, Test Loss: 0.202, Test Acc: 0.868\n",
      "Epoch: 040, Train Loss: 0.344, Train Acc: 0.867, Test Loss: 0.197, Test Acc: 0.842\n",
      "Epoch: 041, Train Loss: 0.337, Train Acc: 0.867, Test Loss: 0.200, Test Acc: 0.868\n",
      "Epoch: 042, Train Loss: 0.339, Train Acc: 0.867, Test Loss: 0.195, Test Acc: 0.842\n",
      "Epoch: 043, Train Loss: 0.332, Train Acc: 0.867, Test Loss: 0.197, Test Acc: 0.868\n",
      "Epoch: 044, Train Loss: 0.351, Train Acc: 0.873, Test Loss: 0.196, Test Acc: 0.868\n",
      "Epoch: 045, Train Loss: 0.341, Train Acc: 0.867, Test Loss: 0.192, Test Acc: 0.842\n",
      "Epoch: 046, Train Loss: 0.332, Train Acc: 0.867, Test Loss: 0.194, Test Acc: 0.868\n",
      "Epoch: 047, Train Loss: 0.331, Train Acc: 0.867, Test Loss: 0.195, Test Acc: 0.842\n",
      "Epoch: 048, Train Loss: 0.332, Train Acc: 0.867, Test Loss: 0.202, Test Acc: 0.842\n",
      "Epoch: 049, Train Loss: 0.329, Train Acc: 0.867, Test Loss: 0.198, Test Acc: 0.868\n",
      "RUN:4,Test Acc:0.8684\n",
      "Epoch: 001, Train Loss: 0.650, Train Acc: 0.667, Test Loss: 0.458, Test Acc: 0.658\n",
      "Epoch: 002, Train Loss: 0.589, Train Acc: 0.667, Test Loss: 0.457, Test Acc: 0.658\n",
      "Epoch: 003, Train Loss: 0.567, Train Acc: 0.667, Test Loss: 0.460, Test Acc: 0.658\n",
      "Epoch: 004, Train Loss: 0.532, Train Acc: 0.667, Test Loss: 0.410, Test Acc: 0.658\n",
      "Epoch: 005, Train Loss: 0.509, Train Acc: 0.707, Test Loss: 0.416, Test Acc: 0.737\n",
      "Epoch: 006, Train Loss: 0.485, Train Acc: 0.760, Test Loss: 0.412, Test Acc: 0.921\n",
      "Epoch: 007, Train Loss: 0.464, Train Acc: 0.760, Test Loss: 0.374, Test Acc: 0.921\n",
      "Epoch: 008, Train Loss: 0.463, Train Acc: 0.807, Test Loss: 0.395, Test Acc: 0.947\n",
      "Epoch: 009, Train Loss: 0.434, Train Acc: 0.807, Test Loss: 0.378, Test Acc: 0.921\n",
      "Epoch: 010, Train Loss: 0.443, Train Acc: 0.813, Test Loss: 0.367, Test Acc: 0.921\n",
      "Epoch: 011, Train Loss: 0.416, Train Acc: 0.813, Test Loss: 0.388, Test Acc: 0.947\n",
      "Epoch: 012, Train Loss: 0.415, Train Acc: 0.813, Test Loss: 0.364, Test Acc: 0.947\n",
      "Epoch: 013, Train Loss: 0.410, Train Acc: 0.827, Test Loss: 0.360, Test Acc: 0.947\n",
      "Epoch: 014, Train Loss: 0.418, Train Acc: 0.827, Test Loss: 0.339, Test Acc: 0.947\n",
      "Epoch: 015, Train Loss: 0.406, Train Acc: 0.827, Test Loss: 0.358, Test Acc: 0.974\n",
      "Epoch: 016, Train Loss: 0.407, Train Acc: 0.813, Test Loss: 0.307, Test Acc: 0.947\n",
      "Epoch: 017, Train Loss: 0.401, Train Acc: 0.833, Test Loss: 0.346, Test Acc: 0.974\n",
      "Epoch: 018, Train Loss: 0.397, Train Acc: 0.833, Test Loss: 0.358, Test Acc: 0.974\n",
      "Epoch: 019, Train Loss: 0.397, Train Acc: 0.833, Test Loss: 0.316, Test Acc: 0.974\n",
      "Epoch: 020, Train Loss: 0.394, Train Acc: 0.827, Test Loss: 0.293, Test Acc: 0.974\n",
      "Epoch: 021, Train Loss: 0.394, Train Acc: 0.833, Test Loss: 0.340, Test Acc: 0.947\n",
      "Epoch: 022, Train Loss: 0.389, Train Acc: 0.833, Test Loss: 0.317, Test Acc: 0.974\n",
      "Epoch: 023, Train Loss: 0.389, Train Acc: 0.833, Test Loss: 0.316, Test Acc: 0.974\n",
      "Epoch: 024, Train Loss: 0.394, Train Acc: 0.847, Test Loss: 0.346, Test Acc: 0.947\n",
      "Epoch: 025, Train Loss: 0.393, Train Acc: 0.833, Test Loss: 0.337, Test Acc: 0.947\n",
      "Epoch: 026, Train Loss: 0.386, Train Acc: 0.833, Test Loss: 0.297, Test Acc: 0.974\n",
      "Epoch: 027, Train Loss: 0.400, Train Acc: 0.833, Test Loss: 0.400, Test Acc: 0.947\n",
      "Epoch: 028, Train Loss: 0.405, Train Acc: 0.833, Test Loss: 0.311, Test Acc: 0.947\n",
      "Epoch: 029, Train Loss: 0.385, Train Acc: 0.853, Test Loss: 0.422, Test Acc: 0.895\n",
      "Epoch: 030, Train Loss: 0.385, Train Acc: 0.833, Test Loss: 0.299, Test Acc: 0.974\n",
      "Epoch: 031, Train Loss: 0.387, Train Acc: 0.833, Test Loss: 0.334, Test Acc: 0.947\n",
      "Epoch: 032, Train Loss: 0.384, Train Acc: 0.847, Test Loss: 0.336, Test Acc: 0.947\n",
      "Epoch: 033, Train Loss: 0.382, Train Acc: 0.847, Test Loss: 0.357, Test Acc: 0.947\n",
      "Epoch: 034, Train Loss: 0.394, Train Acc: 0.833, Test Loss: 0.325, Test Acc: 0.947\n",
      "Epoch: 035, Train Loss: 0.387, Train Acc: 0.847, Test Loss: 0.343, Test Acc: 0.947\n",
      "Epoch: 036, Train Loss: 0.386, Train Acc: 0.847, Test Loss: 0.363, Test Acc: 0.947\n",
      "Epoch: 037, Train Loss: 0.385, Train Acc: 0.840, Test Loss: 0.327, Test Acc: 0.947\n",
      "Epoch: 038, Train Loss: 0.385, Train Acc: 0.847, Test Loss: 0.388, Test Acc: 0.947\n",
      "Epoch: 039, Train Loss: 0.380, Train Acc: 0.847, Test Loss: 0.352, Test Acc: 0.947\n",
      "Epoch: 040, Train Loss: 0.382, Train Acc: 0.847, Test Loss: 0.356, Test Acc: 0.947\n",
      "Epoch: 041, Train Loss: 0.378, Train Acc: 0.847, Test Loss: 0.376, Test Acc: 0.947\n",
      "Epoch: 042, Train Loss: 0.379, Train Acc: 0.833, Test Loss: 0.333, Test Acc: 0.895\n",
      "Epoch: 043, Train Loss: 0.396, Train Acc: 0.847, Test Loss: 0.385, Test Acc: 0.895\n",
      "Epoch: 044, Train Loss: 0.422, Train Acc: 0.847, Test Loss: 0.387, Test Acc: 0.895\n",
      "Epoch: 045, Train Loss: 0.400, Train Acc: 0.813, Test Loss: 0.277, Test Acc: 0.895\n",
      "Epoch: 046, Train Loss: 0.385, Train Acc: 0.827, Test Loss: 0.546, Test Acc: 0.816\n",
      "Epoch: 047, Train Loss: 0.401, Train Acc: 0.833, Test Loss: 0.327, Test Acc: 0.895\n",
      "Epoch: 048, Train Loss: 0.409, Train Acc: 0.840, Test Loss: 0.307, Test Acc: 0.895\n",
      "Epoch: 049, Train Loss: 0.396, Train Acc: 0.827, Test Loss: 0.499, Test Acc: 0.816\n",
      "RUN:5,Test Acc:0.8158\n",
      "Epoch: 001, Train Loss: 0.579, Train Acc: 0.660, Test Loss: 0.514, Test Acc: 0.684\n",
      "Epoch: 002, Train Loss: 0.508, Train Acc: 0.733, Test Loss: 0.489, Test Acc: 0.737\n",
      "Epoch: 003, Train Loss: 0.458, Train Acc: 0.833, Test Loss: 0.483, Test Acc: 0.842\n",
      "Epoch: 004, Train Loss: 0.423, Train Acc: 0.833, Test Loss: 0.484, Test Acc: 0.842\n",
      "Epoch: 005, Train Loss: 0.398, Train Acc: 0.827, Test Loss: 0.508, Test Acc: 0.842\n",
      "Epoch: 006, Train Loss: 0.383, Train Acc: 0.847, Test Loss: 0.521, Test Acc: 0.842\n",
      "Epoch: 007, Train Loss: 0.376, Train Acc: 0.827, Test Loss: 0.566, Test Acc: 0.842\n",
      "Epoch: 008, Train Loss: 0.370, Train Acc: 0.867, Test Loss: 0.537, Test Acc: 0.842\n",
      "Epoch: 009, Train Loss: 0.359, Train Acc: 0.860, Test Loss: 0.572, Test Acc: 0.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Loss: 0.357, Train Acc: 0.860, Test Loss: 0.593, Test Acc: 0.842\n",
      "Epoch: 011, Train Loss: 0.363, Train Acc: 0.867, Test Loss: 0.609, Test Acc: 0.842\n",
      "Epoch: 012, Train Loss: 0.356, Train Acc: 0.860, Test Loss: 0.649, Test Acc: 0.842\n",
      "Epoch: 013, Train Loss: 0.357, Train Acc: 0.860, Test Loss: 0.607, Test Acc: 0.842\n",
      "Epoch: 014, Train Loss: 0.352, Train Acc: 0.873, Test Loss: 0.614, Test Acc: 0.842\n",
      "Epoch: 015, Train Loss: 0.364, Train Acc: 0.860, Test Loss: 0.635, Test Acc: 0.842\n",
      "Epoch: 016, Train Loss: 0.369, Train Acc: 0.860, Test Loss: 0.637, Test Acc: 0.842\n",
      "Epoch: 017, Train Loss: 0.354, Train Acc: 0.873, Test Loss: 0.602, Test Acc: 0.842\n",
      "Epoch: 018, Train Loss: 0.351, Train Acc: 0.867, Test Loss: 0.666, Test Acc: 0.816\n",
      "Epoch: 019, Train Loss: 0.379, Train Acc: 0.873, Test Loss: 0.596, Test Acc: 0.842\n",
      "Epoch: 020, Train Loss: 0.363, Train Acc: 0.867, Test Loss: 0.686, Test Acc: 0.816\n",
      "Epoch: 021, Train Loss: 0.363, Train Acc: 0.867, Test Loss: 0.576, Test Acc: 0.842\n",
      "Epoch: 022, Train Loss: 0.344, Train Acc: 0.860, Test Loss: 0.647, Test Acc: 0.816\n",
      "Epoch: 023, Train Loss: 0.355, Train Acc: 0.873, Test Loss: 0.606, Test Acc: 0.842\n",
      "Epoch: 024, Train Loss: 0.365, Train Acc: 0.860, Test Loss: 0.640, Test Acc: 0.842\n",
      "Epoch: 025, Train Loss: 0.351, Train Acc: 0.860, Test Loss: 0.576, Test Acc: 0.842\n",
      "Epoch: 026, Train Loss: 0.346, Train Acc: 0.860, Test Loss: 0.664, Test Acc: 0.816\n",
      "Epoch: 027, Train Loss: 0.343, Train Acc: 0.873, Test Loss: 0.606, Test Acc: 0.842\n",
      "Epoch: 028, Train Loss: 0.362, Train Acc: 0.873, Test Loss: 0.627, Test Acc: 0.816\n",
      "Epoch: 029, Train Loss: 0.363, Train Acc: 0.860, Test Loss: 0.570, Test Acc: 0.842\n",
      "Epoch: 030, Train Loss: 0.358, Train Acc: 0.867, Test Loss: 0.667, Test Acc: 0.816\n",
      "Epoch: 031, Train Loss: 0.348, Train Acc: 0.860, Test Loss: 0.559, Test Acc: 0.842\n",
      "Epoch: 032, Train Loss: 0.359, Train Acc: 0.860, Test Loss: 0.614, Test Acc: 0.816\n",
      "Epoch: 033, Train Loss: 0.346, Train Acc: 0.873, Test Loss: 0.597, Test Acc: 0.842\n",
      "Epoch: 034, Train Loss: 0.347, Train Acc: 0.867, Test Loss: 0.614, Test Acc: 0.816\n",
      "Epoch: 035, Train Loss: 0.347, Train Acc: 0.873, Test Loss: 0.588, Test Acc: 0.842\n",
      "Epoch: 036, Train Loss: 0.348, Train Acc: 0.867, Test Loss: 0.664, Test Acc: 0.816\n",
      "Epoch: 037, Train Loss: 0.339, Train Acc: 0.833, Test Loss: 0.572, Test Acc: 0.816\n",
      "Epoch: 038, Train Loss: 0.344, Train Acc: 0.860, Test Loss: 0.620, Test Acc: 0.816\n",
      "Epoch: 039, Train Loss: 0.355, Train Acc: 0.860, Test Loss: 0.631, Test Acc: 0.816\n",
      "Epoch: 040, Train Loss: 0.340, Train Acc: 0.860, Test Loss: 0.582, Test Acc: 0.842\n",
      "Epoch: 041, Train Loss: 0.339, Train Acc: 0.860, Test Loss: 0.636, Test Acc: 0.816\n",
      "Epoch: 042, Train Loss: 0.340, Train Acc: 0.860, Test Loss: 0.634, Test Acc: 0.816\n",
      "Epoch: 043, Train Loss: 0.348, Train Acc: 0.873, Test Loss: 0.610, Test Acc: 0.816\n",
      "Epoch: 044, Train Loss: 0.340, Train Acc: 0.860, Test Loss: 0.639, Test Acc: 0.816\n",
      "Epoch: 045, Train Loss: 0.343, Train Acc: 0.873, Test Loss: 0.617, Test Acc: 0.816\n",
      "Epoch: 046, Train Loss: 0.338, Train Acc: 0.873, Test Loss: 0.646, Test Acc: 0.816\n",
      "Epoch: 047, Train Loss: 0.340, Train Acc: 0.873, Test Loss: 0.633, Test Acc: 0.816\n",
      "Epoch: 048, Train Loss: 0.342, Train Acc: 0.873, Test Loss: 0.634, Test Acc: 0.816\n",
      "Epoch: 049, Train Loss: 0.351, Train Acc: 0.873, Test Loss: 0.600, Test Acc: 0.816\n",
      "RUN:6,Test Acc:0.8158\n",
      "Epoch: 001, Train Loss: 0.604, Train Acc: 0.653, Test Loss: 0.497, Test Acc: 0.711\n",
      "Epoch: 002, Train Loss: 0.557, Train Acc: 0.653, Test Loss: 0.476, Test Acc: 0.711\n",
      "Epoch: 003, Train Loss: 0.524, Train Acc: 0.720, Test Loss: 0.441, Test Acc: 0.763\n",
      "Epoch: 004, Train Loss: 0.489, Train Acc: 0.800, Test Loss: 0.413, Test Acc: 0.842\n",
      "Epoch: 005, Train Loss: 0.460, Train Acc: 0.813, Test Loss: 0.382, Test Acc: 0.868\n",
      "Epoch: 006, Train Loss: 0.436, Train Acc: 0.813, Test Loss: 0.365, Test Acc: 0.868\n",
      "Epoch: 007, Train Loss: 0.421, Train Acc: 0.820, Test Loss: 0.353, Test Acc: 0.868\n",
      "Epoch: 008, Train Loss: 0.422, Train Acc: 0.827, Test Loss: 0.347, Test Acc: 0.868\n",
      "Epoch: 009, Train Loss: 0.421, Train Acc: 0.813, Test Loss: 0.359, Test Acc: 0.868\n",
      "Epoch: 010, Train Loss: 0.438, Train Acc: 0.867, Test Loss: 0.361, Test Acc: 0.842\n",
      "Epoch: 011, Train Loss: 0.380, Train Acc: 0.827, Test Loss: 0.377, Test Acc: 0.868\n",
      "Epoch: 012, Train Loss: 0.382, Train Acc: 0.860, Test Loss: 0.350, Test Acc: 0.842\n",
      "Epoch: 013, Train Loss: 0.370, Train Acc: 0.860, Test Loss: 0.351, Test Acc: 0.842\n",
      "Epoch: 014, Train Loss: 0.365, Train Acc: 0.860, Test Loss: 0.363, Test Acc: 0.868\n",
      "Epoch: 015, Train Loss: 0.356, Train Acc: 0.873, Test Loss: 0.354, Test Acc: 0.842\n",
      "Epoch: 016, Train Loss: 0.348, Train Acc: 0.860, Test Loss: 0.367, Test Acc: 0.842\n",
      "Epoch: 017, Train Loss: 0.348, Train Acc: 0.860, Test Loss: 0.370, Test Acc: 0.842\n",
      "Epoch: 018, Train Loss: 0.349, Train Acc: 0.873, Test Loss: 0.366, Test Acc: 0.842\n",
      "Epoch: 019, Train Loss: 0.348, Train Acc: 0.860, Test Loss: 0.392, Test Acc: 0.842\n",
      "Epoch: 020, Train Loss: 0.346, Train Acc: 0.873, Test Loss: 0.381, Test Acc: 0.842\n",
      "Epoch: 021, Train Loss: 0.337, Train Acc: 0.860, Test Loss: 0.402, Test Acc: 0.842\n",
      "Epoch: 022, Train Loss: 0.336, Train Acc: 0.873, Test Loss: 0.393, Test Acc: 0.842\n",
      "Epoch: 023, Train Loss: 0.331, Train Acc: 0.860, Test Loss: 0.409, Test Acc: 0.842\n",
      "Epoch: 024, Train Loss: 0.337, Train Acc: 0.873, Test Loss: 0.408, Test Acc: 0.842\n",
      "Epoch: 025, Train Loss: 0.338, Train Acc: 0.873, Test Loss: 0.416, Test Acc: 0.842\n",
      "Epoch: 026, Train Loss: 0.333, Train Acc: 0.860, Test Loss: 0.433, Test Acc: 0.842\n",
      "Epoch: 027, Train Loss: 0.332, Train Acc: 0.873, Test Loss: 0.416, Test Acc: 0.816\n",
      "Epoch: 028, Train Loss: 0.333, Train Acc: 0.873, Test Loss: 0.422, Test Acc: 0.789\n",
      "Epoch: 029, Train Loss: 0.341, Train Acc: 0.873, Test Loss: 0.447, Test Acc: 0.842\n",
      "Epoch: 030, Train Loss: 0.325, Train Acc: 0.860, Test Loss: 0.437, Test Acc: 0.763\n",
      "Epoch: 031, Train Loss: 0.334, Train Acc: 0.860, Test Loss: 0.473, Test Acc: 0.842\n",
      "Epoch: 032, Train Loss: 0.322, Train Acc: 0.833, Test Loss: 0.448, Test Acc: 0.737\n",
      "Epoch: 033, Train Loss: 0.329, Train Acc: 0.873, Test Loss: 0.467, Test Acc: 0.789\n",
      "Epoch: 034, Train Loss: 0.331, Train Acc: 0.873, Test Loss: 0.471, Test Acc: 0.789\n",
      "Epoch: 035, Train Loss: 0.322, Train Acc: 0.873, Test Loss: 0.471, Test Acc: 0.789\n",
      "Epoch: 036, Train Loss: 0.325, Train Acc: 0.873, Test Loss: 0.486, Test Acc: 0.842\n",
      "Epoch: 037, Train Loss: 0.327, Train Acc: 0.873, Test Loss: 0.482, Test Acc: 0.789\n",
      "Epoch: 038, Train Loss: 0.325, Train Acc: 0.873, Test Loss: 0.476, Test Acc: 0.789\n",
      "Epoch: 039, Train Loss: 0.323, Train Acc: 0.867, Test Loss: 0.500, Test Acc: 0.842\n",
      "Epoch: 040, Train Loss: 0.323, Train Acc: 0.873, Test Loss: 0.493, Test Acc: 0.816\n",
      "Epoch: 041, Train Loss: 0.326, Train Acc: 0.867, Test Loss: 0.474, Test Acc: 0.737\n",
      "Epoch: 042, Train Loss: 0.354, Train Acc: 0.853, Test Loss: 0.517, Test Acc: 0.816\n",
      "Epoch: 043, Train Loss: 0.334, Train Acc: 0.833, Test Loss: 0.461, Test Acc: 0.789\n",
      "Epoch: 044, Train Loss: 0.346, Train Acc: 0.853, Test Loss: 0.518, Test Acc: 0.816\n",
      "Epoch: 045, Train Loss: 0.322, Train Acc: 0.867, Test Loss: 0.462, Test Acc: 0.737\n",
      "Epoch: 046, Train Loss: 0.326, Train Acc: 0.867, Test Loss: 0.478, Test Acc: 0.789\n",
      "Epoch: 047, Train Loss: 0.333, Train Acc: 0.867, Test Loss: 0.485, Test Acc: 0.816\n",
      "Epoch: 048, Train Loss: 0.317, Train Acc: 0.853, Test Loss: 0.459, Test Acc: 0.737\n",
      "Epoch: 049, Train Loss: 0.317, Train Acc: 0.860, Test Loss: 0.496, Test Acc: 0.789\n",
      "RUN:7,Test Acc:0.7895\n",
      "Epoch: 001, Train Loss: 0.591, Train Acc: 0.667, Test Loss: 0.570, Test Acc: 0.658\n",
      "Epoch: 002, Train Loss: 0.523, Train Acc: 0.720, Test Loss: 0.485, Test Acc: 0.737\n",
      "Epoch: 003, Train Loss: 0.501, Train Acc: 0.760, Test Loss: 0.428, Test Acc: 0.842\n",
      "Epoch: 004, Train Loss: 0.465, Train Acc: 0.773, Test Loss: 0.384, Test Acc: 0.868\n",
      "Epoch: 005, Train Loss: 0.445, Train Acc: 0.807, Test Loss: 0.318, Test Acc: 0.921\n",
      "Epoch: 006, Train Loss: 0.427, Train Acc: 0.800, Test Loss: 0.284, Test Acc: 0.921\n",
      "Epoch: 007, Train Loss: 0.423, Train Acc: 0.800, Test Loss: 0.269, Test Acc: 0.921\n",
      "Epoch: 008, Train Loss: 0.420, Train Acc: 0.820, Test Loss: 0.238, Test Acc: 0.947\n",
      "Epoch: 009, Train Loss: 0.406, Train Acc: 0.820, Test Loss: 0.230, Test Acc: 0.947\n",
      "Epoch: 010, Train Loss: 0.406, Train Acc: 0.820, Test Loss: 0.227, Test Acc: 0.947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Train Loss: 0.394, Train Acc: 0.827, Test Loss: 0.217, Test Acc: 0.947\n",
      "Epoch: 012, Train Loss: 0.400, Train Acc: 0.827, Test Loss: 0.211, Test Acc: 0.947\n",
      "Epoch: 013, Train Loss: 0.393, Train Acc: 0.820, Test Loss: 0.218, Test Acc: 0.947\n",
      "Epoch: 014, Train Loss: 0.411, Train Acc: 0.847, Test Loss: 0.202, Test Acc: 0.974\n",
      "Epoch: 015, Train Loss: 0.397, Train Acc: 0.827, Test Loss: 0.209, Test Acc: 0.947\n",
      "Epoch: 016, Train Loss: 0.393, Train Acc: 0.833, Test Loss: 0.202, Test Acc: 0.974\n",
      "Epoch: 017, Train Loss: 0.389, Train Acc: 0.833, Test Loss: 0.195, Test Acc: 0.974\n",
      "Epoch: 018, Train Loss: 0.390, Train Acc: 0.847, Test Loss: 0.195, Test Acc: 0.974\n",
      "Epoch: 019, Train Loss: 0.384, Train Acc: 0.833, Test Loss: 0.197, Test Acc: 0.974\n",
      "Epoch: 020, Train Loss: 0.387, Train Acc: 0.833, Test Loss: 0.198, Test Acc: 0.974\n",
      "Epoch: 021, Train Loss: 0.388, Train Acc: 0.847, Test Loss: 0.191, Test Acc: 0.974\n",
      "Epoch: 022, Train Loss: 0.384, Train Acc: 0.840, Test Loss: 0.191, Test Acc: 0.974\n",
      "Epoch: 023, Train Loss: 0.382, Train Acc: 0.833, Test Loss: 0.195, Test Acc: 0.974\n",
      "Epoch: 024, Train Loss: 0.382, Train Acc: 0.840, Test Loss: 0.193, Test Acc: 0.895\n",
      "Epoch: 025, Train Loss: 0.391, Train Acc: 0.833, Test Loss: 0.203, Test Acc: 0.974\n",
      "Epoch: 026, Train Loss: 0.381, Train Acc: 0.833, Test Loss: 0.194, Test Acc: 0.974\n",
      "Epoch: 027, Train Loss: 0.380, Train Acc: 0.840, Test Loss: 0.191, Test Acc: 0.974\n",
      "Epoch: 028, Train Loss: 0.378, Train Acc: 0.840, Test Loss: 0.189, Test Acc: 0.895\n",
      "Epoch: 029, Train Loss: 0.383, Train Acc: 0.833, Test Loss: 0.202, Test Acc: 0.974\n",
      "Epoch: 030, Train Loss: 0.388, Train Acc: 0.840, Test Loss: 0.194, Test Acc: 0.974\n",
      "Epoch: 031, Train Loss: 0.381, Train Acc: 0.840, Test Loss: 0.198, Test Acc: 0.974\n",
      "Epoch: 032, Train Loss: 0.374, Train Acc: 0.827, Test Loss: 0.202, Test Acc: 0.974\n",
      "Epoch: 033, Train Loss: 0.372, Train Acc: 0.840, Test Loss: 0.193, Test Acc: 0.947\n",
      "Epoch: 034, Train Loss: 0.376, Train Acc: 0.840, Test Loss: 0.197, Test Acc: 0.974\n",
      "Epoch: 035, Train Loss: 0.378, Train Acc: 0.840, Test Loss: 0.191, Test Acc: 0.974\n",
      "Epoch: 036, Train Loss: 0.390, Train Acc: 0.827, Test Loss: 0.188, Test Acc: 0.974\n",
      "Epoch: 037, Train Loss: 0.375, Train Acc: 0.873, Test Loss: 0.192, Test Acc: 0.842\n",
      "Epoch: 038, Train Loss: 0.387, Train Acc: 0.840, Test Loss: 0.198, Test Acc: 0.974\n",
      "Epoch: 039, Train Loss: 0.380, Train Acc: 0.840, Test Loss: 0.195, Test Acc: 0.974\n",
      "Epoch: 040, Train Loss: 0.389, Train Acc: 0.833, Test Loss: 0.211, Test Acc: 0.974\n",
      "Epoch: 041, Train Loss: 0.363, Train Acc: 0.860, Test Loss: 0.192, Test Acc: 0.763\n",
      "Epoch: 042, Train Loss: 0.376, Train Acc: 0.833, Test Loss: 0.196, Test Acc: 0.974\n",
      "Epoch: 043, Train Loss: 0.380, Train Acc: 0.827, Test Loss: 0.198, Test Acc: 0.974\n",
      "Epoch: 044, Train Loss: 0.385, Train Acc: 0.840, Test Loss: 0.188, Test Acc: 0.947\n",
      "Epoch: 045, Train Loss: 0.391, Train Acc: 0.827, Test Loss: 0.196, Test Acc: 0.974\n",
      "Epoch: 046, Train Loss: 0.378, Train Acc: 0.853, Test Loss: 0.194, Test Acc: 0.763\n",
      "Epoch: 047, Train Loss: 0.369, Train Acc: 0.833, Test Loss: 0.210, Test Acc: 0.974\n",
      "Epoch: 048, Train Loss: 0.387, Train Acc: 0.840, Test Loss: 0.197, Test Acc: 0.947\n",
      "Epoch: 049, Train Loss: 0.387, Train Acc: 0.847, Test Loss: 0.190, Test Acc: 0.868\n",
      "RUN:8,Test Acc:0.8684\n",
      "Epoch: 001, Train Loss: 0.678, Train Acc: 0.660, Test Loss: 0.542, Test Acc: 0.684\n",
      "Epoch: 002, Train Loss: 0.601, Train Acc: 0.660, Test Loss: 0.476, Test Acc: 0.684\n",
      "Epoch: 003, Train Loss: 0.545, Train Acc: 0.660, Test Loss: 0.456, Test Acc: 0.684\n",
      "Epoch: 004, Train Loss: 0.504, Train Acc: 0.733, Test Loss: 0.435, Test Acc: 0.711\n",
      "Epoch: 005, Train Loss: 0.475, Train Acc: 0.813, Test Loss: 0.426, Test Acc: 0.789\n",
      "Epoch: 006, Train Loss: 0.444, Train Acc: 0.820, Test Loss: 0.419, Test Acc: 0.842\n",
      "Epoch: 007, Train Loss: 0.418, Train Acc: 0.820, Test Loss: 0.416, Test Acc: 0.842\n",
      "Epoch: 008, Train Loss: 0.398, Train Acc: 0.840, Test Loss: 0.441, Test Acc: 0.868\n",
      "Epoch: 009, Train Loss: 0.382, Train Acc: 0.840, Test Loss: 0.405, Test Acc: 0.868\n",
      "Epoch: 010, Train Loss: 0.372, Train Acc: 0.840, Test Loss: 0.411, Test Acc: 0.868\n",
      "Epoch: 011, Train Loss: 0.379, Train Acc: 0.860, Test Loss: 0.453, Test Acc: 0.868\n",
      "Epoch: 012, Train Loss: 0.359, Train Acc: 0.853, Test Loss: 0.423, Test Acc: 0.868\n",
      "Epoch: 013, Train Loss: 0.370, Train Acc: 0.860, Test Loss: 0.436, Test Acc: 0.868\n",
      "Epoch: 014, Train Loss: 0.377, Train Acc: 0.860, Test Loss: 0.429, Test Acc: 0.868\n",
      "Epoch: 015, Train Loss: 0.350, Train Acc: 0.853, Test Loss: 0.386, Test Acc: 0.868\n",
      "Epoch: 016, Train Loss: 0.349, Train Acc: 0.860, Test Loss: 0.524, Test Acc: 0.868\n",
      "Epoch: 017, Train Loss: 0.352, Train Acc: 0.860, Test Loss: 0.404, Test Acc: 0.868\n",
      "Epoch: 018, Train Loss: 0.356, Train Acc: 0.853, Test Loss: 0.444, Test Acc: 0.868\n",
      "Epoch: 019, Train Loss: 0.367, Train Acc: 0.853, Test Loss: 0.438, Test Acc: 0.868\n",
      "Epoch: 020, Train Loss: 0.370, Train Acc: 0.860, Test Loss: 0.403, Test Acc: 0.868\n",
      "Epoch: 021, Train Loss: 0.350, Train Acc: 0.867, Test Loss: 0.503, Test Acc: 0.868\n",
      "Epoch: 022, Train Loss: 0.345, Train Acc: 0.853, Test Loss: 0.435, Test Acc: 0.868\n",
      "Epoch: 023, Train Loss: 0.353, Train Acc: 0.867, Test Loss: 0.447, Test Acc: 0.868\n",
      "Epoch: 024, Train Loss: 0.345, Train Acc: 0.867, Test Loss: 0.451, Test Acc: 0.868\n",
      "Epoch: 025, Train Loss: 0.342, Train Acc: 0.853, Test Loss: 0.423, Test Acc: 0.868\n",
      "Epoch: 026, Train Loss: 0.346, Train Acc: 0.867, Test Loss: 0.442, Test Acc: 0.868\n",
      "Epoch: 027, Train Loss: 0.341, Train Acc: 0.867, Test Loss: 0.473, Test Acc: 0.868\n",
      "Epoch: 028, Train Loss: 0.342, Train Acc: 0.867, Test Loss: 0.444, Test Acc: 0.868\n",
      "Epoch: 029, Train Loss: 0.343, Train Acc: 0.867, Test Loss: 0.451, Test Acc: 0.868\n",
      "Epoch: 030, Train Loss: 0.337, Train Acc: 0.867, Test Loss: 0.507, Test Acc: 0.868\n",
      "Epoch: 031, Train Loss: 0.342, Train Acc: 0.860, Test Loss: 0.442, Test Acc: 0.868\n",
      "Epoch: 032, Train Loss: 0.354, Train Acc: 0.867, Test Loss: 0.501, Test Acc: 0.868\n",
      "Epoch: 033, Train Loss: 0.380, Train Acc: 0.867, Test Loss: 0.458, Test Acc: 0.868\n",
      "Epoch: 034, Train Loss: 0.393, Train Acc: 0.860, Test Loss: 0.386, Test Acc: 0.868\n",
      "Epoch: 035, Train Loss: 0.351, Train Acc: 0.840, Test Loss: 0.581, Test Acc: 0.789\n",
      "Epoch: 036, Train Loss: 0.332, Train Acc: 0.840, Test Loss: 0.368, Test Acc: 0.842\n",
      "Epoch: 037, Train Loss: 0.380, Train Acc: 0.853, Test Loss: 0.422, Test Acc: 0.868\n",
      "Epoch: 038, Train Loss: 0.360, Train Acc: 0.867, Test Loss: 0.515, Test Acc: 0.868\n",
      "Epoch: 039, Train Loss: 0.338, Train Acc: 0.853, Test Loss: 0.405, Test Acc: 0.868\n",
      "Epoch: 040, Train Loss: 0.342, Train Acc: 0.867, Test Loss: 0.499, Test Acc: 0.868\n",
      "Epoch: 041, Train Loss: 0.335, Train Acc: 0.853, Test Loss: 0.435, Test Acc: 0.868\n",
      "Epoch: 042, Train Loss: 0.348, Train Acc: 0.853, Test Loss: 0.423, Test Acc: 0.868\n",
      "Epoch: 043, Train Loss: 0.340, Train Acc: 0.867, Test Loss: 0.518, Test Acc: 0.868\n",
      "Epoch: 044, Train Loss: 0.341, Train Acc: 0.867, Test Loss: 0.448, Test Acc: 0.868\n",
      "Epoch: 045, Train Loss: 0.332, Train Acc: 0.867, Test Loss: 0.495, Test Acc: 0.868\n",
      "Epoch: 046, Train Loss: 0.336, Train Acc: 0.867, Test Loss: 0.464, Test Acc: 0.868\n",
      "Epoch: 047, Train Loss: 0.333, Train Acc: 0.867, Test Loss: 0.483, Test Acc: 0.868\n",
      "Epoch: 048, Train Loss: 0.334, Train Acc: 0.867, Test Loss: 0.473, Test Acc: 0.868\n",
      "Epoch: 049, Train Loss: 0.338, Train Acc: 0.867, Test Loss: 0.463, Test Acc: 0.868\n",
      "RUN:9,Test Acc:0.8684\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    k = 0\n",
    "    nG = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        #print(data.x.size(), data.edge_index.size(),data.batch.size(), k)\n",
    "        data\n",
    "        nG += data.num_graphs\n",
    "        optimizer.zero_grad()\n",
    "        out= model(data.x, data.edge_index, data.batch) # data.batch  torch.Size([783])\n",
    "        loss = F.nll_loss(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * loss.item()\n",
    "        optimizer.step()\n",
    "        k = k + 1\n",
    "    #print(\"Training graphs per epoch\", nG)\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(pred, data.y.view(-1))\n",
    "        correct += pred.max(dim=1)[1].eq(data.y.view(-1)).sum().item()\n",
    "\n",
    "    return loss, correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = start_patience = 50\n",
    "resultados = []\n",
    "for i in range(10):\n",
    "    dataset = dataset.shuffle()\n",
    "    train_dataset = dataset[:150]\n",
    "    test_dataset = dataset[150:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    for epoch in range(1, 50):\n",
    "        train_loss = train(epoch)\n",
    "        _, train_acc = test(train_loader)\n",
    "        test_loss, test_acc = test(test_loader)\n",
    "        print('Epoch: {:03d}, '\n",
    "              'Train Loss: {:.3f}, Train Acc: {:.3f}, '\n",
    "              'Test Loss: {:.3f}, Test Acc: {:.3f}'.format(epoch, train_loss,\n",
    "                                                           train_acc,\n",
    "                                                            test_loss,\n",
    "                                                           test_acc))\n",
    "    print(f'RUN:{i},Test Acc:{test_acc:.4f}')\n",
    "    resultados.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93558f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8342)\n",
      "tensor(0.0412)\n"
     ]
    }
   ],
   "source": [
    "res= torch.Tensor(resultados)\n",
    "print(torch.mean(res))\n",
    "print(torch.std(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da0a9d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 46], x=[22, 7], edge_attr=[46, 4], y=[1], batch=[22], ptr=[2])\n",
      "tensor([0])\n",
      "tensor([1])\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset[2:], batch_size=1, shuffle=False)\n",
    "sample = next(iter(test_loader))\n",
    "#sample.batch =torch.tensor(1)\n",
    "print(sample)\n",
    "print(sample.y)\n",
    "out = model(sample.x,sample.edge_index, sample.batch)  # Perform a single forward pass.\n",
    "pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "print(pred)\n",
    "correct = 0\n",
    "correct += int((pred[0] == sample[0].y).sum())  # Check against ground-truth labels.\n",
    "print(\"Accuracy: \",(correct/1)*100)#Batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80db837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 32])\n",
      "torch.Size([1, 22, 32])\n",
      "[0]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "#https://www.geeksforgeeks.org/network-centrality-measures-in-a-graph-using-networkx-python/\n",
    "#Nodos criticos\n",
    "import networkx as nx\n",
    "Conv_out= model.salidaConv\n",
    "print(model.salidaConv.shape)\n",
    "critical_nodes = Conv_out.argmax(0).unique().tolist()\n",
    "#Pool_out = model.salidaPooling\n",
    "print(Conv_out.shape)\n",
    "#print(Pool_out.shape)\n",
    "print(critical_nodes)\n",
    "#print(Conv_out)\n",
    "grafo_init = sample\n",
    "A = to_networkx(grafo_init, to_undirected=True)\n",
    "deg_centrality = list(nx.closeness_centrality(A))\n",
    "print(torch.Tensor(deg_centrality).argsort(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff40bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 46], x=[22, 7], edge_attr=[46, 4], y=[1], batch=[22], ptr=[2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#loader =  DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "grafo_init = sample\n",
    "print(grafo_init)\n",
    "A = to_networkx(grafo_init, to_undirected=True)\n",
    "l=[]\n",
    "for a in A.nodes:\n",
    "    l.append(a)\n",
    "ed= []\n",
    "for e in A.edges:\n",
    "    ed.append(e)\n",
    "import igraph as ig\n",
    "import chart_studio.plotly\n",
    "Edges= ed\n",
    "G=ig.Graph(Edges, directed=False)\n",
    "labels= l\n",
    "#groups = A.node_attr_dict_factory\n",
    "N = len(A.nodes)\n",
    "#print(N)\n",
    "layt=G.layout('kk', dim=3)\n",
    "#print(layt)\n",
    "Xn=[layt[k][0] for k in range(N)]# x-coordinates of nodes\n",
    "Yn=[layt[k][1] for k in range(N)]# y-coordinates\n",
    "Zn=[layt[k][2] for k in range(N)]# z-coordinates\n",
    "Xe=[]\n",
    "Ye=[]\n",
    "Ze=[]\n",
    "#print(Edges)\n",
    "for e in Edges:\n",
    "    Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends\n",
    "    Ye+=[layt[e[0]][1],layt[e[1]][1], None]\n",
    "    Ze+=[layt[e[0]][2],layt[e[1]][2], None]\n",
    "nx.draw(A)\n",
    "\n",
    "\n",
    "node_adjacencies = []\n",
    "node_text = []\n",
    "for node, adjacencies in enumerate(A.nodes()):\n",
    "    #print(adjacencies,critical_nodes)\n",
    "    if adjacencies in critical_nodes :\n",
    "        node_adjacencies.append('rgb(256,0,0)')\n",
    "    else:\n",
    "        node_adjacencies.append('rgb(0,0,0)')\n",
    "    #node_text.append('# of connections: '+str(len(adjacencies[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81331e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chart_studio import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1=go.Scatter3d(x=Xe,\n",
    "               y=Ye,\n",
    "               z=Ze,\n",
    "               mode='lines',\n",
    "               line=dict(color='rgb(125,125,125)', width=1),\n",
    "               hoverinfo='none'\n",
    "               )\n",
    "\n",
    "trace2=go.Scatter3d(x=Xn,\n",
    "               y=Yn,\n",
    "               z=Zn,\n",
    "               mode='markers',\n",
    "               name='actors',\n",
    "               marker=dict(symbol='circle',\n",
    "                             size=6,\n",
    "                             #color='#ff7f0e',\n",
    "                             colorscale='Viridis',\n",
    "                             line=dict(color='rgb(50,50,50)', width=0.5)\n",
    "                             ),\n",
    "               text=labels,\n",
    "               hoverinfo='text'\n",
    "               )\n",
    "trace2.marker.color = node_adjacencies\n",
    "#print(trace2)\n",
    "axis=dict(showbackground=False,\n",
    "          showline=False,\n",
    "          zeroline=False,\n",
    "          showgrid=False,\n",
    "          showticklabels=False,\n",
    "          title=''\n",
    "          )\n",
    "\n",
    "layout = go.Layout(\n",
    "         title=\"3D graph visualization of Mutag dataset  by : Ahmed\",\n",
    "         width=1000,\n",
    "         height=1000,\n",
    "         showlegend=False,\n",
    "         scene=dict(\n",
    "             xaxis=dict(axis),\n",
    "             yaxis=dict(axis),\n",
    "             zaxis=dict(axis),\n",
    "        ),\n",
    "     margin=dict(\n",
    "        t=100\n",
    "    ),\n",
    "    hovermode='closest',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7781d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "none",
         "line": {
          "color": "rgb(125,125,125)",
          "width": 1
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          -0.6833924098088815,
          -0.8434978518070027,
          null,
          -0.6833924098088815,
          -0.3552398799461058,
          null,
          -0.8434978518070027,
          -0.7350874913237152,
          null,
          -0.7350874913237152,
          -0.4078439885652504,
          null,
          -0.7350874913237152,
          -0.8950485033940883,
          null,
          -0.4078439885652504,
          -0.24702132578195152,
          null,
          -0.24702132578195152,
          -0.3552398799461058,
          null,
          -0.3552398799461058,
          -0.195256703888728,
          null,
          -0.195256703888728,
          -0.03199957084757259,
          null,
          -0.03199957084757259,
          0.151842841087223,
          null,
          0.151842841087223,
          0.30416048742307333,
          null,
          0.151842841087223,
          0.22005824097404808,
          null,
          0.22005824097404808,
          0.538430622340736,
          null,
          0.22005824097404808,
          0.03984609902228274,
          null,
          0.538430622340736,
          0.6441068668793322,
          null,
          0.6441068668793322,
          0.46908155089051146,
          null,
          0.46908155089051146,
          0.14255681967849818,
          null,
          0.46908155089051146,
          0.5810743348324671,
          null,
          0.14255681967849818,
          0.03984609902228274,
          null,
          0.5810743348324671,
          0.771508570819895,
          null,
          0.5810743348324671,
          0.54060808021499,
          null,
          -0.8950485033940883,
          -0.8468747610301599,
          null,
          -0.8950485033940883,
          -1.1524919638934625,
          null
         ],
         "y": [
          0.10925305513906242,
          0.005204155419075869,
          null,
          0.10925305513906242,
          -0.011236733497911898,
          null,
          0.005204155419075869,
          -0.257915639692108,
          null,
          -0.257915639692108,
          -0.37957366651024477,
          null,
          -0.257915639692108,
          -0.35974937035186394,
          null,
          -0.37957366651024477,
          -0.2747862386698969,
          null,
          -0.2747862386698969,
          -0.011236733497911898,
          null,
          -0.011236733497911898,
          0.09611737290467447,
          null,
          0.09611737290467447,
          0.22141235050098546,
          null,
          0.22141235050098546,
          0.39484785584789284,
          null,
          0.39484785584789284,
          0.7398489744790616,
          null,
          0.39484785584789284,
          0.3032194917562301,
          null,
          0.3032194917562301,
          0.15167329312918984,
          null,
          0.3032194917562301,
          0.4064851782293146,
          null,
          0.15167329312918984,
          0.128639405569923,
          null,
          0.128639405569923,
          0.24917124844165273,
          null,
          0.24917124844165273,
          0.3837733781945901,
          null,
          0.24917124844165273,
          0.24345212620469833,
          null,
          0.3837733781945901,
          0.4064851782293146,
          null,
          0.24345212620469833,
          0.492099213898832,
          null,
          0.24345212620469833,
          -0.010919087254668045,
          null,
          -0.35974937035186394,
          -0.2344946762050493,
          null,
          -0.35974937035186394,
          -0.6199883292891213,
          null
         ],
         "z": [
          -1.0829954700522746,
          -1.408696432022637,
          null,
          -1.0829954700522746,
          -0.9076946124341975,
          null,
          -1.408696432022637,
          -1.677070002310617,
          null,
          -1.677070002310617,
          -1.5009486093676279,
          null,
          -1.677070002310617,
          -2.004686909406984,
          null,
          -1.5009486093676279,
          -1.1756135255663678,
          null,
          -1.1756135255663678,
          -0.9076946124341975,
          null,
          -0.9076946124341975,
          -0.5947410975870563,
          null,
          -0.5947410975870563,
          -0.2996810587489618,
          null,
          -0.2996810587489618,
          -0.018701718129664206,
          null,
          -0.018701718129664206,
          -0.13565010105006942,
          null,
          -0.018701718129664206,
          0.354249409012158,
          null,
          0.354249409012158,
          0.5316282911215546,
          null,
          0.354249409012158,
          0.690024254513631,
          null,
          0.5316282911215546,
          0.8957047198895216,
          null,
          0.8957047198895216,
          1.2247735231159345,
          null,
          1.2247735231159345,
          1.0551902722695392,
          null,
          1.2247735231159345,
          1.5866326652499965,
          null,
          1.0551902722695392,
          0.690024254513631,
          null,
          1.5866326652499965,
          1.7941679449052033,
          null,
          1.5866326652499965,
          1.860063203161882,
          null,
          -2.004686909406984,
          -2.3554678424298,
          null,
          -2.004686909406984,
          -2.0878744549074937,
          null
         ]
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": [
           "rgb(256,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)",
           "rgb(0,0,0)"
          ],
          "colorscale": [
           [
            0,
            "#440154"
           ],
           [
            0.1111111111111111,
            "#482878"
           ],
           [
            0.2222222222222222,
            "#3e4989"
           ],
           [
            0.3333333333333333,
            "#31688e"
           ],
           [
            0.4444444444444444,
            "#26828e"
           ],
           [
            0.5555555555555556,
            "#1f9e89"
           ],
           [
            0.6666666666666666,
            "#35b779"
           ],
           [
            0.7777777777777778,
            "#6ece58"
           ],
           [
            0.8888888888888888,
            "#b5de2b"
           ],
           [
            1,
            "#fde725"
           ]
          ],
          "line": {
           "color": "rgb(50,50,50)",
           "width": 0.5
          },
          "size": 6,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "actors",
         "text": [
          "0",
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21"
         ],
         "type": "scatter3d",
         "x": [
          -0.6833924098088815,
          -0.8434978518070027,
          -0.7350874913237152,
          -0.4078439885652504,
          -0.24702132578195152,
          -0.3552398799461058,
          -0.195256703888728,
          -0.03199957084757259,
          0.151842841087223,
          0.30416048742307333,
          0.22005824097404808,
          0.538430622340736,
          0.6441068668793322,
          0.46908155089051146,
          0.14255681967849818,
          0.03984609902228274,
          0.5810743348324671,
          0.771508570819895,
          0.54060808021499,
          -0.8950485033940883,
          -0.8468747610301599,
          -1.1524919638934625
         ],
         "y": [
          0.10925305513906242,
          0.005204155419075869,
          -0.257915639692108,
          -0.37957366651024477,
          -0.2747862386698969,
          -0.011236733497911898,
          0.09611737290467447,
          0.22141235050098546,
          0.39484785584789284,
          0.7398489744790616,
          0.3032194917562301,
          0.15167329312918984,
          0.128639405569923,
          0.24917124844165273,
          0.3837733781945901,
          0.4064851782293146,
          0.24345212620469833,
          0.492099213898832,
          -0.010919087254668045,
          -0.35974937035186394,
          -0.2344946762050493,
          -0.6199883292891213
         ],
         "z": [
          -1.0829954700522746,
          -1.408696432022637,
          -1.677070002310617,
          -1.5009486093676279,
          -1.1756135255663678,
          -0.9076946124341975,
          -0.5947410975870563,
          -0.2996810587489618,
          -0.018701718129664206,
          -0.13565010105006942,
          0.354249409012158,
          0.5316282911215546,
          0.8957047198895216,
          1.2247735231159345,
          1.0551902722695392,
          0.690024254513631,
          1.5866326652499965,
          1.7941679449052033,
          1.860063203161882,
          -2.004686909406984,
          -2.3554678424298,
          -2.0878744549074937
         ]
        }
       ],
       "layout": {
        "height": 1000,
        "hovermode": "closest",
        "margin": {
         "t": 100
        },
        "scene": {
         "xaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": {
           "text": ""
          },
          "zeroline": false
         },
         "yaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": {
           "text": ""
          },
          "zeroline": false
         },
         "zaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": {
           "text": ""
          },
          "zeroline": false
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "3D graph visualization of Mutag dataset  by : Ahmed"
        },
        "width": 1000
       }
      },
      "text/html": [
       "<div>                            <div id=\"85affbf8-91c8-4886-b03f-c43706b13165\" class=\"plotly-graph-div\" style=\"height:1000px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"85affbf8-91c8-4886-b03f-c43706b13165\")) {                    Plotly.newPlot(                        \"85affbf8-91c8-4886-b03f-c43706b13165\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"rgb(125,125,125)\",\"width\":1},\"mode\":\"lines\",\"x\":[-0.6833924098088815,-0.8434978518070027,null,-0.6833924098088815,-0.3552398799461058,null,-0.8434978518070027,-0.7350874913237152,null,-0.7350874913237152,-0.4078439885652504,null,-0.7350874913237152,-0.8950485033940883,null,-0.4078439885652504,-0.24702132578195152,null,-0.24702132578195152,-0.3552398799461058,null,-0.3552398799461058,-0.195256703888728,null,-0.195256703888728,-0.03199957084757259,null,-0.03199957084757259,0.151842841087223,null,0.151842841087223,0.30416048742307333,null,0.151842841087223,0.22005824097404808,null,0.22005824097404808,0.538430622340736,null,0.22005824097404808,0.03984609902228274,null,0.538430622340736,0.6441068668793322,null,0.6441068668793322,0.46908155089051146,null,0.46908155089051146,0.14255681967849818,null,0.46908155089051146,0.5810743348324671,null,0.14255681967849818,0.03984609902228274,null,0.5810743348324671,0.771508570819895,null,0.5810743348324671,0.54060808021499,null,-0.8950485033940883,-0.8468747610301599,null,-0.8950485033940883,-1.1524919638934625,null],\"y\":[0.10925305513906242,0.005204155419075869,null,0.10925305513906242,-0.011236733497911898,null,0.005204155419075869,-0.257915639692108,null,-0.257915639692108,-0.37957366651024477,null,-0.257915639692108,-0.35974937035186394,null,-0.37957366651024477,-0.2747862386698969,null,-0.2747862386698969,-0.011236733497911898,null,-0.011236733497911898,0.09611737290467447,null,0.09611737290467447,0.22141235050098546,null,0.22141235050098546,0.39484785584789284,null,0.39484785584789284,0.7398489744790616,null,0.39484785584789284,0.3032194917562301,null,0.3032194917562301,0.15167329312918984,null,0.3032194917562301,0.4064851782293146,null,0.15167329312918984,0.128639405569923,null,0.128639405569923,0.24917124844165273,null,0.24917124844165273,0.3837733781945901,null,0.24917124844165273,0.24345212620469833,null,0.3837733781945901,0.4064851782293146,null,0.24345212620469833,0.492099213898832,null,0.24345212620469833,-0.010919087254668045,null,-0.35974937035186394,-0.2344946762050493,null,-0.35974937035186394,-0.6199883292891213,null],\"z\":[-1.0829954700522746,-1.408696432022637,null,-1.0829954700522746,-0.9076946124341975,null,-1.408696432022637,-1.677070002310617,null,-1.677070002310617,-1.5009486093676279,null,-1.677070002310617,-2.004686909406984,null,-1.5009486093676279,-1.1756135255663678,null,-1.1756135255663678,-0.9076946124341975,null,-0.9076946124341975,-0.5947410975870563,null,-0.5947410975870563,-0.2996810587489618,null,-0.2996810587489618,-0.018701718129664206,null,-0.018701718129664206,-0.13565010105006942,null,-0.018701718129664206,0.354249409012158,null,0.354249409012158,0.5316282911215546,null,0.354249409012158,0.690024254513631,null,0.5316282911215546,0.8957047198895216,null,0.8957047198895216,1.2247735231159345,null,1.2247735231159345,1.0551902722695392,null,1.2247735231159345,1.5866326652499965,null,1.0551902722695392,0.690024254513631,null,1.5866326652499965,1.7941679449052033,null,1.5866326652499965,1.860063203161882,null,-2.004686909406984,-2.3554678424298,null,-2.004686909406984,-2.0878744549074937,null],\"type\":\"scatter3d\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":[\"rgb(256,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\",\"rgb(0,0,0)\"],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"line\":{\"color\":\"rgb(50,50,50)\",\"width\":0.5},\"size\":6,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"actors\",\"text\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\"],\"x\":[-0.6833924098088815,-0.8434978518070027,-0.7350874913237152,-0.4078439885652504,-0.24702132578195152,-0.3552398799461058,-0.195256703888728,-0.03199957084757259,0.151842841087223,0.30416048742307333,0.22005824097404808,0.538430622340736,0.6441068668793322,0.46908155089051146,0.14255681967849818,0.03984609902228274,0.5810743348324671,0.771508570819895,0.54060808021499,-0.8950485033940883,-0.8468747610301599,-1.1524919638934625],\"y\":[0.10925305513906242,0.005204155419075869,-0.257915639692108,-0.37957366651024477,-0.2747862386698969,-0.011236733497911898,0.09611737290467447,0.22141235050098546,0.39484785584789284,0.7398489744790616,0.3032194917562301,0.15167329312918984,0.128639405569923,0.24917124844165273,0.3837733781945901,0.4064851782293146,0.24345212620469833,0.492099213898832,-0.010919087254668045,-0.35974937035186394,-0.2344946762050493,-0.6199883292891213],\"z\":[-1.0829954700522746,-1.408696432022637,-1.677070002310617,-1.5009486093676279,-1.1756135255663678,-0.9076946124341975,-0.5947410975870563,-0.2996810587489618,-0.018701718129664206,-0.13565010105006942,0.354249409012158,0.5316282911215546,0.8957047198895216,1.2247735231159345,1.0551902722695392,0.690024254513631,1.5866326652499965,1.7941679449052033,1.860063203161882,-2.004686909406984,-2.3554678424298,-2.0878744549074937],\"type\":\"scatter3d\"}],                        {\"height\":1000,\"hovermode\":\"closest\",\"margin\":{\"t\":100},\"scene\":{\"xaxis\":{\"showbackground\":false,\"showgrid\":false,\"showline\":false,\"showticklabels\":false,\"title\":{\"text\":\"\"},\"zeroline\":false},\"yaxis\":{\"showbackground\":false,\"showgrid\":false,\"showline\":false,\"showticklabels\":false,\"title\":{\"text\":\"\"},\"zeroline\":false},\"zaxis\":{\"showbackground\":false,\"showgrid\":false,\"showline\":false,\"showticklabels\":false,\"title\":{\"text\":\"\"},\"zeroline\":false}},\"showlegend\":false,\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"3D graph visualization of Mutag dataset  by : Ahmed\"},\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('85affbf8-91c8-4886-b03f-c43706b13165');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import plotly.graph_objs as go\n",
    "from  plotly.offline import plot\n",
    "import chart_studio.plotly as py\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\n",
    "init_notebook_mode(connected='true')\n",
    "data=[trace1, trace2]\n",
    "fig=go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='Les-Miserables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c537632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PROTEINS(1113):\n",
      "====================\n",
      "Number of graphs: 1113\n",
      "Number of features: 3\n",
      "Number of classes: 2\n",
      "\n",
      "Data(edge_index=[2, 162], x=[42, 3], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 42\n",
      "Number of edges: 162\n",
      "Average node degree: 3.86\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='data/TUDataset', name='PROTEINS')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3449bf4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 890\n",
      "Number of test graphs: 223\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:890]\n",
    "test_dataset = dataset[890:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f8f4ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 712], x=[329, 7], edge_attr=[712, 4], y=[20], batch=[329], ptr=[21])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 870], x=[389, 7], edge_attr=[870, 4], y=[20], batch=[389], ptr=[21])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 784], x=[355, 7], edge_attr=[784, 4], y=[20], batch=[355], ptr=[21])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 792], x=[360, 7], edge_attr=[792, 4], y=[20], batch=[360], ptr=[21])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 754], x=[343, 7], edge_attr=[754, 4], y=[20], batch=[343], ptr=[21])\n",
      "\n",
      "Step 6:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 842], x=[382, 7], edge_attr=[842, 4], y=[20], batch=[382], ptr=[21])\n",
      "\n",
      "Step 7:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(edge_index=[2, 762], x=[347, 7], edge_attr=[762, 4], y=[20], batch=[347], ptr=[21])\n",
      "\n",
      "Step 8:\n",
      "=======\n",
      "Number of graphs in the current batch: 10\n",
      "DataBatch(edge_index=[2, 426], x=[190, 7], edge_attr=[426, 4], y=[10], batch=[190], ptr=[11])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "545de6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.593, Train Acc: 0.747, Test Loss: 0.651, Test Acc: 0.767\n",
      "Epoch: 002, Train Loss: 0.545, Train Acc: 0.742, Test Loss: 0.576, Test Acc: 0.749\n",
      "Epoch: 003, Train Loss: 0.529, Train Acc: 0.753, Test Loss: 0.579, Test Acc: 0.749\n",
      "Epoch: 004, Train Loss: 0.521, Train Acc: 0.757, Test Loss: 0.557, Test Acc: 0.776\n",
      "Epoch: 005, Train Loss: 0.516, Train Acc: 0.757, Test Loss: 0.585, Test Acc: 0.762\n",
      "Epoch: 006, Train Loss: 0.514, Train Acc: 0.751, Test Loss: 0.559, Test Acc: 0.785\n",
      "Epoch: 007, Train Loss: 0.514, Train Acc: 0.758, Test Loss: 0.567, Test Acc: 0.776\n",
      "Epoch: 008, Train Loss: 0.513, Train Acc: 0.762, Test Loss: 0.552, Test Acc: 0.771\n",
      "Epoch: 009, Train Loss: 0.503, Train Acc: 0.740, Test Loss: 0.576, Test Acc: 0.785\n",
      "Epoch: 010, Train Loss: 0.506, Train Acc: 0.754, Test Loss: 0.557, Test Acc: 0.771\n",
      "Epoch: 011, Train Loss: 0.501, Train Acc: 0.754, Test Loss: 0.564, Test Acc: 0.771\n",
      "Epoch: 012, Train Loss: 0.500, Train Acc: 0.758, Test Loss: 0.549, Test Acc: 0.762\n",
      "Epoch: 013, Train Loss: 0.500, Train Acc: 0.747, Test Loss: 0.556, Test Acc: 0.794\n",
      "Epoch: 014, Train Loss: 0.497, Train Acc: 0.760, Test Loss: 0.562, Test Acc: 0.780\n",
      "Epoch: 015, Train Loss: 0.502, Train Acc: 0.747, Test Loss: 0.536, Test Acc: 0.735\n",
      "Epoch: 016, Train Loss: 0.497, Train Acc: 0.762, Test Loss: 0.587, Test Acc: 0.789\n",
      "Epoch: 017, Train Loss: 0.494, Train Acc: 0.753, Test Loss: 0.518, Test Acc: 0.753\n",
      "Epoch: 018, Train Loss: 0.503, Train Acc: 0.767, Test Loss: 0.560, Test Acc: 0.785\n",
      "Epoch: 019, Train Loss: 0.496, Train Acc: 0.747, Test Loss: 0.582, Test Acc: 0.794\n",
      "Epoch: 020, Train Loss: 0.501, Train Acc: 0.765, Test Loss: 0.579, Test Acc: 0.785\n",
      "Epoch: 021, Train Loss: 0.493, Train Acc: 0.766, Test Loss: 0.536, Test Acc: 0.785\n",
      "Epoch: 022, Train Loss: 0.494, Train Acc: 0.752, Test Loss: 0.598, Test Acc: 0.785\n",
      "Epoch: 023, Train Loss: 0.489, Train Acc: 0.761, Test Loss: 0.535, Test Acc: 0.749\n",
      "Epoch: 024, Train Loss: 0.508, Train Acc: 0.761, Test Loss: 0.568, Test Acc: 0.785\n",
      "Epoch: 025, Train Loss: 0.507, Train Acc: 0.746, Test Loss: 0.606, Test Acc: 0.780\n",
      "Epoch: 026, Train Loss: 0.507, Train Acc: 0.765, Test Loss: 0.542, Test Acc: 0.785\n",
      "Epoch: 027, Train Loss: 0.492, Train Acc: 0.765, Test Loss: 0.553, Test Acc: 0.758\n",
      "Epoch: 028, Train Loss: 0.493, Train Acc: 0.757, Test Loss: 0.571, Test Acc: 0.780\n",
      "Epoch: 029, Train Loss: 0.497, Train Acc: 0.764, Test Loss: 0.564, Test Acc: 0.785\n",
      "Epoch: 030, Train Loss: 0.493, Train Acc: 0.766, Test Loss: 0.563, Test Acc: 0.789\n",
      "Epoch: 031, Train Loss: 0.492, Train Acc: 0.765, Test Loss: 0.544, Test Acc: 0.785\n",
      "Epoch: 032, Train Loss: 0.489, Train Acc: 0.766, Test Loss: 0.554, Test Acc: 0.776\n",
      "Epoch: 033, Train Loss: 0.489, Train Acc: 0.756, Test Loss: 0.575, Test Acc: 0.776\n",
      "Epoch: 034, Train Loss: 0.492, Train Acc: 0.770, Test Loss: 0.557, Test Acc: 0.780\n",
      "Epoch: 035, Train Loss: 0.491, Train Acc: 0.767, Test Loss: 0.541, Test Acc: 0.785\n",
      "Epoch: 036, Train Loss: 0.486, Train Acc: 0.767, Test Loss: 0.572, Test Acc: 0.785\n",
      "Epoch: 037, Train Loss: 0.486, Train Acc: 0.766, Test Loss: 0.551, Test Acc: 0.785\n",
      "Epoch: 038, Train Loss: 0.488, Train Acc: 0.767, Test Loss: 0.543, Test Acc: 0.776\n",
      "Epoch: 039, Train Loss: 0.487, Train Acc: 0.769, Test Loss: 0.535, Test Acc: 0.776\n",
      "Epoch: 040, Train Loss: 0.487, Train Acc: 0.755, Test Loss: 0.564, Test Acc: 0.780\n",
      "Epoch: 041, Train Loss: 0.487, Train Acc: 0.767, Test Loss: 0.548, Test Acc: 0.785\n",
      "Epoch: 042, Train Loss: 0.489, Train Acc: 0.765, Test Loss: 0.537, Test Acc: 0.785\n",
      "Epoch: 043, Train Loss: 0.486, Train Acc: 0.772, Test Loss: 0.536, Test Acc: 0.758\n",
      "Epoch: 044, Train Loss: 0.492, Train Acc: 0.767, Test Loss: 0.537, Test Acc: 0.780\n",
      "Epoch: 045, Train Loss: 0.485, Train Acc: 0.771, Test Loss: 0.552, Test Acc: 0.776\n",
      "Epoch: 046, Train Loss: 0.484, Train Acc: 0.765, Test Loss: 0.568, Test Acc: 0.785\n",
      "Epoch: 047, Train Loss: 0.490, Train Acc: 0.748, Test Loss: 0.559, Test Acc: 0.789\n",
      "Epoch: 048, Train Loss: 0.494, Train Acc: 0.762, Test Loss: 0.523, Test Acc: 0.753\n",
      "Epoch: 049, Train Loss: 0.509, Train Acc: 0.761, Test Loss: 0.577, Test Acc: 0.789\n",
      "RUN:0,Test Acc:0.7892\n",
      "Epoch: 001, Train Loss: 0.827, Train Acc: 0.638, Test Loss: 0.630, Test Acc: 0.700\n",
      "Epoch: 002, Train Loss: 0.595, Train Acc: 0.730, Test Loss: 0.578, Test Acc: 0.753\n",
      "Epoch: 003, Train Loss: 0.558, Train Acc: 0.718, Test Loss: 0.577, Test Acc: 0.785\n",
      "Epoch: 004, Train Loss: 0.549, Train Acc: 0.744, Test Loss: 0.556, Test Acc: 0.767\n",
      "Epoch: 005, Train Loss: 0.534, Train Acc: 0.749, Test Loss: 0.545, Test Acc: 0.767\n",
      "Epoch: 006, Train Loss: 0.526, Train Acc: 0.756, Test Loss: 0.540, Test Acc: 0.762\n",
      "Epoch: 007, Train Loss: 0.520, Train Acc: 0.760, Test Loss: 0.539, Test Acc: 0.771\n",
      "Epoch: 008, Train Loss: 0.517, Train Acc: 0.761, Test Loss: 0.535, Test Acc: 0.771\n",
      "Epoch: 009, Train Loss: 0.512, Train Acc: 0.746, Test Loss: 0.540, Test Acc: 0.776\n",
      "Epoch: 010, Train Loss: 0.518, Train Acc: 0.744, Test Loss: 0.559, Test Acc: 0.807\n",
      "Epoch: 011, Train Loss: 0.518, Train Acc: 0.757, Test Loss: 0.540, Test Acc: 0.762\n",
      "Epoch: 012, Train Loss: 0.513, Train Acc: 0.766, Test Loss: 0.517, Test Acc: 0.789\n",
      "Epoch: 013, Train Loss: 0.511, Train Acc: 0.758, Test Loss: 0.532, Test Acc: 0.758\n",
      "Epoch: 014, Train Loss: 0.503, Train Acc: 0.756, Test Loss: 0.528, Test Acc: 0.803\n",
      "Epoch: 015, Train Loss: 0.504, Train Acc: 0.773, Test Loss: 0.521, Test Acc: 0.780\n",
      "Epoch: 016, Train Loss: 0.501, Train Acc: 0.769, Test Loss: 0.520, Test Acc: 0.780\n",
      "Epoch: 017, Train Loss: 0.504, Train Acc: 0.769, Test Loss: 0.514, Test Acc: 0.767\n",
      "Epoch: 018, Train Loss: 0.500, Train Acc: 0.761, Test Loss: 0.512, Test Acc: 0.771\n",
      "Epoch: 019, Train Loss: 0.500, Train Acc: 0.762, Test Loss: 0.523, Test Acc: 0.758\n",
      "Epoch: 020, Train Loss: 0.497, Train Acc: 0.755, Test Loss: 0.517, Test Acc: 0.794\n",
      "Epoch: 021, Train Loss: 0.499, Train Acc: 0.762, Test Loss: 0.505, Test Acc: 0.762\n",
      "Epoch: 022, Train Loss: 0.500, Train Acc: 0.757, Test Loss: 0.488, Test Acc: 0.771\n",
      "Epoch: 023, Train Loss: 0.515, Train Acc: 0.748, Test Loss: 0.557, Test Acc: 0.803\n",
      "Epoch: 024, Train Loss: 0.504, Train Acc: 0.766, Test Loss: 0.535, Test Acc: 0.771\n",
      "Epoch: 025, Train Loss: 0.503, Train Acc: 0.770, Test Loss: 0.518, Test Acc: 0.771\n",
      "Epoch: 026, Train Loss: 0.506, Train Acc: 0.748, Test Loss: 0.547, Test Acc: 0.780\n",
      "Epoch: 027, Train Loss: 0.502, Train Acc: 0.762, Test Loss: 0.515, Test Acc: 0.780\n",
      "Epoch: 028, Train Loss: 0.502, Train Acc: 0.765, Test Loss: 0.514, Test Acc: 0.776\n",
      "Epoch: 029, Train Loss: 0.501, Train Acc: 0.765, Test Loss: 0.504, Test Acc: 0.776\n",
      "Epoch: 030, Train Loss: 0.504, Train Acc: 0.766, Test Loss: 0.512, Test Acc: 0.776\n",
      "Epoch: 031, Train Loss: 0.504, Train Acc: 0.770, Test Loss: 0.483, Test Acc: 0.771\n",
      "Epoch: 032, Train Loss: 0.507, Train Acc: 0.752, Test Loss: 0.549, Test Acc: 0.780\n",
      "Epoch: 033, Train Loss: 0.494, Train Acc: 0.762, Test Loss: 0.517, Test Acc: 0.776\n",
      "Epoch: 034, Train Loss: 0.490, Train Acc: 0.749, Test Loss: 0.524, Test Acc: 0.780\n",
      "Epoch: 035, Train Loss: 0.499, Train Acc: 0.772, Test Loss: 0.506, Test Acc: 0.776\n",
      "Epoch: 036, Train Loss: 0.492, Train Acc: 0.749, Test Loss: 0.523, Test Acc: 0.776\n",
      "Epoch: 037, Train Loss: 0.504, Train Acc: 0.757, Test Loss: 0.490, Test Acc: 0.758\n",
      "Epoch: 038, Train Loss: 0.502, Train Acc: 0.749, Test Loss: 0.532, Test Acc: 0.798\n",
      "Epoch: 039, Train Loss: 0.501, Train Acc: 0.769, Test Loss: 0.510, Test Acc: 0.789\n",
      "Epoch: 040, Train Loss: 0.493, Train Acc: 0.775, Test Loss: 0.522, Test Acc: 0.780\n",
      "Epoch: 041, Train Loss: 0.489, Train Acc: 0.771, Test Loss: 0.501, Test Acc: 0.771\n",
      "Epoch: 042, Train Loss: 0.492, Train Acc: 0.770, Test Loss: 0.499, Test Acc: 0.785\n",
      "Epoch: 043, Train Loss: 0.487, Train Acc: 0.771, Test Loss: 0.502, Test Acc: 0.776\n",
      "Epoch: 044, Train Loss: 0.490, Train Acc: 0.769, Test Loss: 0.489, Test Acc: 0.785\n",
      "Epoch: 045, Train Loss: 0.487, Train Acc: 0.769, Test Loss: 0.489, Test Acc: 0.785\n",
      "Epoch: 046, Train Loss: 0.490, Train Acc: 0.764, Test Loss: 0.485, Test Acc: 0.798\n",
      "Epoch: 047, Train Loss: 0.488, Train Acc: 0.770, Test Loss: 0.492, Test Acc: 0.785\n",
      "Epoch: 048, Train Loss: 0.487, Train Acc: 0.771, Test Loss: 0.482, Test Acc: 0.771\n",
      "Epoch: 049, Train Loss: 0.489, Train Acc: 0.763, Test Loss: 0.492, Test Acc: 0.794\n",
      "RUN:1,Test Acc:0.7937\n",
      "Epoch: 001, Train Loss: 0.638, Train Acc: 0.666, Test Loss: 0.529, Test Acc: 0.709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Loss: 0.589, Train Acc: 0.710, Test Loss: 0.508, Test Acc: 0.785\n",
      "Epoch: 003, Train Loss: 0.572, Train Acc: 0.735, Test Loss: 0.481, Test Acc: 0.771\n",
      "Epoch: 004, Train Loss: 0.557, Train Acc: 0.735, Test Loss: 0.467, Test Acc: 0.789\n",
      "Epoch: 005, Train Loss: 0.546, Train Acc: 0.739, Test Loss: 0.471, Test Acc: 0.798\n",
      "Epoch: 006, Train Loss: 0.539, Train Acc: 0.734, Test Loss: 0.485, Test Acc: 0.803\n",
      "Epoch: 007, Train Loss: 0.541, Train Acc: 0.739, Test Loss: 0.489, Test Acc: 0.794\n",
      "Epoch: 008, Train Loss: 0.529, Train Acc: 0.743, Test Loss: 0.467, Test Acc: 0.803\n",
      "Epoch: 009, Train Loss: 0.525, Train Acc: 0.730, Test Loss: 0.476, Test Acc: 0.789\n",
      "Epoch: 010, Train Loss: 0.524, Train Acc: 0.744, Test Loss: 0.459, Test Acc: 0.803\n",
      "Epoch: 011, Train Loss: 0.525, Train Acc: 0.727, Test Loss: 0.461, Test Acc: 0.789\n",
      "Epoch: 012, Train Loss: 0.529, Train Acc: 0.744, Test Loss: 0.484, Test Acc: 0.776\n",
      "Epoch: 013, Train Loss: 0.521, Train Acc: 0.739, Test Loss: 0.467, Test Acc: 0.789\n",
      "Epoch: 014, Train Loss: 0.524, Train Acc: 0.734, Test Loss: 0.464, Test Acc: 0.803\n",
      "Epoch: 015, Train Loss: 0.515, Train Acc: 0.743, Test Loss: 0.449, Test Acc: 0.798\n",
      "Epoch: 016, Train Loss: 0.515, Train Acc: 0.744, Test Loss: 0.456, Test Acc: 0.798\n",
      "Epoch: 017, Train Loss: 0.510, Train Acc: 0.745, Test Loss: 0.460, Test Acc: 0.798\n",
      "Epoch: 018, Train Loss: 0.511, Train Acc: 0.743, Test Loss: 0.446, Test Acc: 0.798\n",
      "Epoch: 019, Train Loss: 0.513, Train Acc: 0.738, Test Loss: 0.457, Test Acc: 0.789\n",
      "Epoch: 020, Train Loss: 0.517, Train Acc: 0.743, Test Loss: 0.468, Test Acc: 0.794\n",
      "Epoch: 021, Train Loss: 0.531, Train Acc: 0.744, Test Loss: 0.507, Test Acc: 0.767\n",
      "Epoch: 022, Train Loss: 0.528, Train Acc: 0.751, Test Loss: 0.432, Test Acc: 0.807\n",
      "Epoch: 023, Train Loss: 0.523, Train Acc: 0.737, Test Loss: 0.464, Test Acc: 0.798\n",
      "Epoch: 024, Train Loss: 0.514, Train Acc: 0.753, Test Loss: 0.478, Test Acc: 0.780\n",
      "Epoch: 025, Train Loss: 0.511, Train Acc: 0.737, Test Loss: 0.466, Test Acc: 0.794\n",
      "Epoch: 026, Train Loss: 0.523, Train Acc: 0.740, Test Loss: 0.474, Test Acc: 0.785\n",
      "Epoch: 027, Train Loss: 0.514, Train Acc: 0.749, Test Loss: 0.446, Test Acc: 0.798\n",
      "Epoch: 028, Train Loss: 0.509, Train Acc: 0.752, Test Loss: 0.453, Test Acc: 0.794\n",
      "Epoch: 029, Train Loss: 0.504, Train Acc: 0.752, Test Loss: 0.456, Test Acc: 0.798\n",
      "Epoch: 030, Train Loss: 0.507, Train Acc: 0.747, Test Loss: 0.461, Test Acc: 0.789\n",
      "Epoch: 031, Train Loss: 0.504, Train Acc: 0.749, Test Loss: 0.452, Test Acc: 0.798\n",
      "Epoch: 032, Train Loss: 0.502, Train Acc: 0.760, Test Loss: 0.490, Test Acc: 0.780\n",
      "Epoch: 033, Train Loss: 0.507, Train Acc: 0.749, Test Loss: 0.449, Test Acc: 0.803\n",
      "Epoch: 034, Train Loss: 0.506, Train Acc: 0.749, Test Loss: 0.447, Test Acc: 0.789\n",
      "Epoch: 035, Train Loss: 0.510, Train Acc: 0.751, Test Loss: 0.481, Test Acc: 0.794\n",
      "Epoch: 036, Train Loss: 0.505, Train Acc: 0.754, Test Loss: 0.487, Test Acc: 0.794\n",
      "Epoch: 037, Train Loss: 0.505, Train Acc: 0.749, Test Loss: 0.456, Test Acc: 0.798\n",
      "Epoch: 038, Train Loss: 0.504, Train Acc: 0.748, Test Loss: 0.490, Test Acc: 0.785\n",
      "Epoch: 039, Train Loss: 0.505, Train Acc: 0.748, Test Loss: 0.492, Test Acc: 0.785\n",
      "Epoch: 040, Train Loss: 0.516, Train Acc: 0.753, Test Loss: 0.481, Test Acc: 0.785\n",
      "Epoch: 041, Train Loss: 0.503, Train Acc: 0.756, Test Loss: 0.484, Test Acc: 0.798\n",
      "Epoch: 042, Train Loss: 0.499, Train Acc: 0.754, Test Loss: 0.488, Test Acc: 0.771\n",
      "Epoch: 043, Train Loss: 0.506, Train Acc: 0.747, Test Loss: 0.448, Test Acc: 0.771\n",
      "Epoch: 044, Train Loss: 0.502, Train Acc: 0.758, Test Loss: 0.442, Test Acc: 0.807\n",
      "Epoch: 045, Train Loss: 0.497, Train Acc: 0.755, Test Loss: 0.497, Test Acc: 0.762\n",
      "Epoch: 046, Train Loss: 0.499, Train Acc: 0.755, Test Loss: 0.465, Test Acc: 0.798\n",
      "Epoch: 047, Train Loss: 0.497, Train Acc: 0.754, Test Loss: 0.488, Test Acc: 0.807\n",
      "Epoch: 048, Train Loss: 0.497, Train Acc: 0.756, Test Loss: 0.503, Test Acc: 0.794\n",
      "Epoch: 049, Train Loss: 0.499, Train Acc: 0.756, Test Loss: 0.483, Test Acc: 0.807\n",
      "RUN:2,Test Acc:0.8072\n",
      "Epoch: 001, Train Loss: 0.646, Train Acc: 0.745, Test Loss: 0.588, Test Acc: 0.749\n",
      "Epoch: 002, Train Loss: 0.551, Train Acc: 0.746, Test Loss: 0.613, Test Acc: 0.753\n",
      "Epoch: 003, Train Loss: 0.531, Train Acc: 0.745, Test Loss: 0.572, Test Acc: 0.758\n",
      "Epoch: 004, Train Loss: 0.518, Train Acc: 0.748, Test Loss: 0.529, Test Acc: 0.771\n",
      "Epoch: 005, Train Loss: 0.511, Train Acc: 0.747, Test Loss: 0.577, Test Acc: 0.771\n",
      "Epoch: 006, Train Loss: 0.507, Train Acc: 0.752, Test Loss: 0.537, Test Acc: 0.762\n",
      "Epoch: 007, Train Loss: 0.503, Train Acc: 0.751, Test Loss: 0.586, Test Acc: 0.753\n",
      "Epoch: 008, Train Loss: 0.498, Train Acc: 0.753, Test Loss: 0.558, Test Acc: 0.776\n",
      "Epoch: 009, Train Loss: 0.495, Train Acc: 0.758, Test Loss: 0.583, Test Acc: 0.767\n",
      "Epoch: 010, Train Loss: 0.496, Train Acc: 0.748, Test Loss: 0.538, Test Acc: 0.762\n",
      "Epoch: 011, Train Loss: 0.494, Train Acc: 0.758, Test Loss: 0.581, Test Acc: 0.767\n",
      "Epoch: 012, Train Loss: 0.494, Train Acc: 0.756, Test Loss: 0.566, Test Acc: 0.771\n",
      "Epoch: 013, Train Loss: 0.497, Train Acc: 0.760, Test Loss: 0.574, Test Acc: 0.762\n",
      "Epoch: 014, Train Loss: 0.495, Train Acc: 0.749, Test Loss: 0.593, Test Acc: 0.749\n",
      "Epoch: 015, Train Loss: 0.495, Train Acc: 0.754, Test Loss: 0.620, Test Acc: 0.753\n",
      "Epoch: 016, Train Loss: 0.499, Train Acc: 0.765, Test Loss: 0.577, Test Acc: 0.780\n",
      "Epoch: 017, Train Loss: 0.491, Train Acc: 0.758, Test Loss: 0.558, Test Acc: 0.776\n",
      "Epoch: 018, Train Loss: 0.497, Train Acc: 0.765, Test Loss: 0.569, Test Acc: 0.780\n",
      "Epoch: 019, Train Loss: 0.496, Train Acc: 0.755, Test Loss: 0.613, Test Acc: 0.753\n",
      "Epoch: 020, Train Loss: 0.495, Train Acc: 0.766, Test Loss: 0.576, Test Acc: 0.767\n",
      "Epoch: 021, Train Loss: 0.489, Train Acc: 0.756, Test Loss: 0.551, Test Acc: 0.780\n",
      "Epoch: 022, Train Loss: 0.500, Train Acc: 0.763, Test Loss: 0.565, Test Acc: 0.762\n",
      "Epoch: 023, Train Loss: 0.488, Train Acc: 0.765, Test Loss: 0.604, Test Acc: 0.762\n",
      "Epoch: 024, Train Loss: 0.487, Train Acc: 0.767, Test Loss: 0.565, Test Acc: 0.780\n",
      "Epoch: 025, Train Loss: 0.487, Train Acc: 0.757, Test Loss: 0.602, Test Acc: 0.767\n",
      "Epoch: 026, Train Loss: 0.484, Train Acc: 0.764, Test Loss: 0.568, Test Acc: 0.785\n",
      "Epoch: 027, Train Loss: 0.485, Train Acc: 0.769, Test Loss: 0.571, Test Acc: 0.776\n",
      "Epoch: 028, Train Loss: 0.486, Train Acc: 0.763, Test Loss: 0.565, Test Acc: 0.780\n",
      "Epoch: 029, Train Loss: 0.483, Train Acc: 0.763, Test Loss: 0.582, Test Acc: 0.762\n",
      "Epoch: 030, Train Loss: 0.483, Train Acc: 0.762, Test Loss: 0.554, Test Acc: 0.776\n",
      "Epoch: 031, Train Loss: 0.483, Train Acc: 0.761, Test Loss: 0.610, Test Acc: 0.762\n",
      "Epoch: 032, Train Loss: 0.483, Train Acc: 0.763, Test Loss: 0.611, Test Acc: 0.749\n",
      "Epoch: 033, Train Loss: 0.487, Train Acc: 0.757, Test Loss: 0.599, Test Acc: 0.776\n",
      "Epoch: 034, Train Loss: 0.486, Train Acc: 0.756, Test Loss: 0.562, Test Acc: 0.771\n",
      "Epoch: 035, Train Loss: 0.495, Train Acc: 0.762, Test Loss: 0.582, Test Acc: 0.776\n",
      "Epoch: 036, Train Loss: 0.487, Train Acc: 0.756, Test Loss: 0.614, Test Acc: 0.744\n",
      "Epoch: 037, Train Loss: 0.486, Train Acc: 0.760, Test Loss: 0.602, Test Acc: 0.767\n",
      "Epoch: 038, Train Loss: 0.485, Train Acc: 0.760, Test Loss: 0.598, Test Acc: 0.776\n",
      "Epoch: 039, Train Loss: 0.482, Train Acc: 0.771, Test Loss: 0.578, Test Acc: 0.771\n",
      "Epoch: 040, Train Loss: 0.482, Train Acc: 0.764, Test Loss: 0.600, Test Acc: 0.767\n",
      "Epoch: 041, Train Loss: 0.477, Train Acc: 0.773, Test Loss: 0.591, Test Acc: 0.771\n",
      "Epoch: 042, Train Loss: 0.480, Train Acc: 0.770, Test Loss: 0.594, Test Acc: 0.767\n",
      "Epoch: 043, Train Loss: 0.479, Train Acc: 0.762, Test Loss: 0.597, Test Acc: 0.758\n",
      "Epoch: 044, Train Loss: 0.478, Train Acc: 0.769, Test Loss: 0.587, Test Acc: 0.771\n",
      "Epoch: 045, Train Loss: 0.484, Train Acc: 0.769, Test Loss: 0.623, Test Acc: 0.758\n",
      "Epoch: 046, Train Loss: 0.478, Train Acc: 0.767, Test Loss: 0.593, Test Acc: 0.767\n",
      "Epoch: 047, Train Loss: 0.485, Train Acc: 0.762, Test Loss: 0.615, Test Acc: 0.753\n",
      "Epoch: 048, Train Loss: 0.479, Train Acc: 0.761, Test Loss: 0.632, Test Acc: 0.758\n",
      "Epoch: 049, Train Loss: 0.479, Train Acc: 0.771, Test Loss: 0.573, Test Acc: 0.767\n",
      "RUN:3,Test Acc:0.7668\n",
      "Epoch: 001, Train Loss: 0.599, Train Acc: 0.710, Test Loss: 0.568, Test Acc: 0.695\n",
      "Epoch: 002, Train Loss: 0.573, Train Acc: 0.727, Test Loss: 0.531, Test Acc: 0.753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Loss: 0.550, Train Acc: 0.731, Test Loss: 0.499, Test Acc: 0.780\n",
      "Epoch: 004, Train Loss: 0.537, Train Acc: 0.743, Test Loss: 0.505, Test Acc: 0.762\n",
      "Epoch: 005, Train Loss: 0.523, Train Acc: 0.754, Test Loss: 0.474, Test Acc: 0.780\n",
      "Epoch: 006, Train Loss: 0.516, Train Acc: 0.760, Test Loss: 0.465, Test Acc: 0.789\n",
      "Epoch: 007, Train Loss: 0.509, Train Acc: 0.748, Test Loss: 0.469, Test Acc: 0.776\n",
      "Epoch: 008, Train Loss: 0.510, Train Acc: 0.753, Test Loss: 0.454, Test Acc: 0.776\n",
      "Epoch: 009, Train Loss: 0.506, Train Acc: 0.751, Test Loss: 0.456, Test Acc: 0.780\n",
      "Epoch: 010, Train Loss: 0.500, Train Acc: 0.753, Test Loss: 0.441, Test Acc: 0.780\n",
      "Epoch: 011, Train Loss: 0.500, Train Acc: 0.756, Test Loss: 0.441, Test Acc: 0.771\n",
      "Epoch: 012, Train Loss: 0.508, Train Acc: 0.737, Test Loss: 0.469, Test Acc: 0.744\n",
      "Epoch: 013, Train Loss: 0.510, Train Acc: 0.758, Test Loss: 0.422, Test Acc: 0.794\n",
      "Epoch: 014, Train Loss: 0.495, Train Acc: 0.761, Test Loss: 0.437, Test Acc: 0.789\n",
      "Epoch: 015, Train Loss: 0.496, Train Acc: 0.753, Test Loss: 0.429, Test Acc: 0.780\n",
      "Epoch: 016, Train Loss: 0.495, Train Acc: 0.757, Test Loss: 0.423, Test Acc: 0.780\n",
      "Epoch: 017, Train Loss: 0.495, Train Acc: 0.765, Test Loss: 0.414, Test Acc: 0.794\n",
      "Epoch: 018, Train Loss: 0.492, Train Acc: 0.758, Test Loss: 0.418, Test Acc: 0.789\n",
      "Epoch: 019, Train Loss: 0.494, Train Acc: 0.758, Test Loss: 0.425, Test Acc: 0.776\n",
      "Epoch: 020, Train Loss: 0.504, Train Acc: 0.758, Test Loss: 0.423, Test Acc: 0.771\n",
      "Epoch: 021, Train Loss: 0.506, Train Acc: 0.754, Test Loss: 0.425, Test Acc: 0.789\n",
      "Epoch: 022, Train Loss: 0.493, Train Acc: 0.766, Test Loss: 0.417, Test Acc: 0.789\n",
      "Epoch: 023, Train Loss: 0.491, Train Acc: 0.760, Test Loss: 0.421, Test Acc: 0.785\n",
      "Epoch: 024, Train Loss: 0.491, Train Acc: 0.767, Test Loss: 0.409, Test Acc: 0.794\n",
      "Epoch: 025, Train Loss: 0.490, Train Acc: 0.762, Test Loss: 0.414, Test Acc: 0.785\n",
      "Epoch: 026, Train Loss: 0.489, Train Acc: 0.769, Test Loss: 0.413, Test Acc: 0.785\n",
      "Epoch: 027, Train Loss: 0.488, Train Acc: 0.761, Test Loss: 0.423, Test Acc: 0.780\n",
      "Epoch: 028, Train Loss: 0.494, Train Acc: 0.763, Test Loss: 0.417, Test Acc: 0.780\n",
      "Epoch: 029, Train Loss: 0.500, Train Acc: 0.752, Test Loss: 0.431, Test Acc: 0.780\n",
      "Epoch: 030, Train Loss: 0.500, Train Acc: 0.767, Test Loss: 0.421, Test Acc: 0.780\n",
      "Epoch: 031, Train Loss: 0.498, Train Acc: 0.763, Test Loss: 0.423, Test Acc: 0.785\n",
      "Epoch: 032, Train Loss: 0.492, Train Acc: 0.766, Test Loss: 0.422, Test Acc: 0.789\n",
      "Epoch: 033, Train Loss: 0.492, Train Acc: 0.762, Test Loss: 0.414, Test Acc: 0.785\n",
      "Epoch: 034, Train Loss: 0.488, Train Acc: 0.763, Test Loss: 0.410, Test Acc: 0.776\n",
      "Epoch: 035, Train Loss: 0.503, Train Acc: 0.756, Test Loss: 0.441, Test Acc: 0.749\n",
      "Epoch: 036, Train Loss: 0.498, Train Acc: 0.767, Test Loss: 0.416, Test Acc: 0.794\n",
      "Epoch: 037, Train Loss: 0.493, Train Acc: 0.763, Test Loss: 0.405, Test Acc: 0.785\n",
      "Epoch: 038, Train Loss: 0.494, Train Acc: 0.758, Test Loss: 0.417, Test Acc: 0.789\n",
      "Epoch: 039, Train Loss: 0.495, Train Acc: 0.765, Test Loss: 0.418, Test Acc: 0.794\n",
      "Epoch: 040, Train Loss: 0.488, Train Acc: 0.775, Test Loss: 0.410, Test Acc: 0.798\n",
      "Epoch: 041, Train Loss: 0.489, Train Acc: 0.767, Test Loss: 0.415, Test Acc: 0.776\n",
      "Epoch: 042, Train Loss: 0.495, Train Acc: 0.766, Test Loss: 0.405, Test Acc: 0.789\n",
      "Epoch: 043, Train Loss: 0.489, Train Acc: 0.771, Test Loss: 0.409, Test Acc: 0.789\n",
      "Epoch: 044, Train Loss: 0.488, Train Acc: 0.763, Test Loss: 0.432, Test Acc: 0.776\n",
      "Epoch: 045, Train Loss: 0.489, Train Acc: 0.754, Test Loss: 0.411, Test Acc: 0.758\n",
      "Epoch: 046, Train Loss: 0.498, Train Acc: 0.769, Test Loss: 0.409, Test Acc: 0.789\n",
      "Epoch: 047, Train Loss: 0.488, Train Acc: 0.769, Test Loss: 0.417, Test Acc: 0.780\n",
      "Epoch: 048, Train Loss: 0.495, Train Acc: 0.756, Test Loss: 0.425, Test Acc: 0.767\n",
      "Epoch: 049, Train Loss: 0.493, Train Acc: 0.770, Test Loss: 0.415, Test Acc: 0.794\n",
      "RUN:4,Test Acc:0.7937\n",
      "Epoch: 001, Train Loss: 0.735, Train Acc: 0.707, Test Loss: 0.567, Test Acc: 0.646\n",
      "Epoch: 002, Train Loss: 0.565, Train Acc: 0.758, Test Loss: 0.539, Test Acc: 0.713\n",
      "Epoch: 003, Train Loss: 0.543, Train Acc: 0.743, Test Loss: 0.522, Test Acc: 0.709\n",
      "Epoch: 004, Train Loss: 0.526, Train Acc: 0.751, Test Loss: 0.521, Test Acc: 0.717\n",
      "Epoch: 005, Train Loss: 0.519, Train Acc: 0.753, Test Loss: 0.521, Test Acc: 0.700\n",
      "Epoch: 006, Train Loss: 0.516, Train Acc: 0.751, Test Loss: 0.499, Test Acc: 0.704\n",
      "Epoch: 007, Train Loss: 0.502, Train Acc: 0.756, Test Loss: 0.502, Test Acc: 0.709\n",
      "Epoch: 008, Train Loss: 0.503, Train Acc: 0.757, Test Loss: 0.490, Test Acc: 0.700\n",
      "Epoch: 009, Train Loss: 0.499, Train Acc: 0.755, Test Loss: 0.489, Test Acc: 0.700\n",
      "Epoch: 010, Train Loss: 0.501, Train Acc: 0.766, Test Loss: 0.489, Test Acc: 0.709\n",
      "Epoch: 011, Train Loss: 0.491, Train Acc: 0.766, Test Loss: 0.489, Test Acc: 0.709\n",
      "Epoch: 012, Train Loss: 0.490, Train Acc: 0.767, Test Loss: 0.486, Test Acc: 0.709\n",
      "Epoch: 013, Train Loss: 0.486, Train Acc: 0.766, Test Loss: 0.480, Test Acc: 0.717\n",
      "Epoch: 014, Train Loss: 0.486, Train Acc: 0.764, Test Loss: 0.482, Test Acc: 0.722\n",
      "Epoch: 015, Train Loss: 0.481, Train Acc: 0.773, Test Loss: 0.476, Test Acc: 0.717\n",
      "Epoch: 016, Train Loss: 0.481, Train Acc: 0.762, Test Loss: 0.479, Test Acc: 0.722\n",
      "Epoch: 017, Train Loss: 0.485, Train Acc: 0.772, Test Loss: 0.478, Test Acc: 0.695\n",
      "Epoch: 018, Train Loss: 0.493, Train Acc: 0.776, Test Loss: 0.482, Test Acc: 0.709\n",
      "Epoch: 019, Train Loss: 0.478, Train Acc: 0.764, Test Loss: 0.475, Test Acc: 0.722\n",
      "Epoch: 020, Train Loss: 0.480, Train Acc: 0.780, Test Loss: 0.487, Test Acc: 0.722\n",
      "Epoch: 021, Train Loss: 0.488, Train Acc: 0.767, Test Loss: 0.511, Test Acc: 0.709\n",
      "Epoch: 022, Train Loss: 0.494, Train Acc: 0.767, Test Loss: 0.479, Test Acc: 0.722\n",
      "Epoch: 023, Train Loss: 0.481, Train Acc: 0.770, Test Loss: 0.477, Test Acc: 0.704\n",
      "Epoch: 024, Train Loss: 0.480, Train Acc: 0.776, Test Loss: 0.490, Test Acc: 0.726\n",
      "Epoch: 025, Train Loss: 0.480, Train Acc: 0.781, Test Loss: 0.488, Test Acc: 0.717\n",
      "Epoch: 026, Train Loss: 0.478, Train Acc: 0.770, Test Loss: 0.478, Test Acc: 0.722\n",
      "Epoch: 027, Train Loss: 0.485, Train Acc: 0.765, Test Loss: 0.496, Test Acc: 0.700\n",
      "Epoch: 028, Train Loss: 0.496, Train Acc: 0.780, Test Loss: 0.495, Test Acc: 0.735\n",
      "Epoch: 029, Train Loss: 0.479, Train Acc: 0.767, Test Loss: 0.481, Test Acc: 0.713\n",
      "Epoch: 030, Train Loss: 0.478, Train Acc: 0.775, Test Loss: 0.489, Test Acc: 0.726\n",
      "Epoch: 031, Train Loss: 0.475, Train Acc: 0.771, Test Loss: 0.482, Test Acc: 0.700\n",
      "Epoch: 032, Train Loss: 0.481, Train Acc: 0.783, Test Loss: 0.484, Test Acc: 0.713\n",
      "Epoch: 033, Train Loss: 0.476, Train Acc: 0.771, Test Loss: 0.486, Test Acc: 0.713\n",
      "Epoch: 034, Train Loss: 0.473, Train Acc: 0.782, Test Loss: 0.491, Test Acc: 0.735\n",
      "Epoch: 035, Train Loss: 0.475, Train Acc: 0.781, Test Loss: 0.488, Test Acc: 0.717\n",
      "Epoch: 036, Train Loss: 0.472, Train Acc: 0.787, Test Loss: 0.487, Test Acc: 0.713\n",
      "Epoch: 037, Train Loss: 0.472, Train Acc: 0.789, Test Loss: 0.487, Test Acc: 0.722\n",
      "Epoch: 038, Train Loss: 0.476, Train Acc: 0.772, Test Loss: 0.487, Test Acc: 0.713\n",
      "Epoch: 039, Train Loss: 0.477, Train Acc: 0.780, Test Loss: 0.489, Test Acc: 0.722\n",
      "Epoch: 040, Train Loss: 0.475, Train Acc: 0.780, Test Loss: 0.492, Test Acc: 0.731\n",
      "Epoch: 041, Train Loss: 0.478, Train Acc: 0.772, Test Loss: 0.488, Test Acc: 0.709\n",
      "Epoch: 042, Train Loss: 0.474, Train Acc: 0.782, Test Loss: 0.499, Test Acc: 0.722\n",
      "Epoch: 043, Train Loss: 0.473, Train Acc: 0.771, Test Loss: 0.487, Test Acc: 0.713\n",
      "Epoch: 044, Train Loss: 0.476, Train Acc: 0.765, Test Loss: 0.485, Test Acc: 0.709\n",
      "Epoch: 045, Train Loss: 0.477, Train Acc: 0.784, Test Loss: 0.498, Test Acc: 0.722\n",
      "Epoch: 046, Train Loss: 0.479, Train Acc: 0.774, Test Loss: 0.504, Test Acc: 0.731\n",
      "Epoch: 047, Train Loss: 0.489, Train Acc: 0.766, Test Loss: 0.488, Test Acc: 0.709\n",
      "Epoch: 048, Train Loss: 0.472, Train Acc: 0.781, Test Loss: 0.495, Test Acc: 0.722\n",
      "Epoch: 049, Train Loss: 0.467, Train Acc: 0.778, Test Loss: 0.490, Test Acc: 0.717\n",
      "RUN:5,Test Acc:0.7175\n",
      "Epoch: 001, Train Loss: 0.654, Train Acc: 0.716, Test Loss: 0.666, Test Acc: 0.717\n",
      "Epoch: 002, Train Loss: 0.566, Train Acc: 0.746, Test Loss: 0.616, Test Acc: 0.749\n",
      "Epoch: 003, Train Loss: 0.548, Train Acc: 0.751, Test Loss: 0.587, Test Acc: 0.749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train Loss: 0.533, Train Acc: 0.755, Test Loss: 0.594, Test Acc: 0.749\n",
      "Epoch: 005, Train Loss: 0.540, Train Acc: 0.751, Test Loss: 0.564, Test Acc: 0.731\n",
      "Epoch: 006, Train Loss: 0.523, Train Acc: 0.755, Test Loss: 0.596, Test Acc: 0.758\n",
      "Epoch: 007, Train Loss: 0.517, Train Acc: 0.758, Test Loss: 0.560, Test Acc: 0.744\n",
      "Epoch: 008, Train Loss: 0.520, Train Acc: 0.752, Test Loss: 0.554, Test Acc: 0.762\n",
      "Epoch: 009, Train Loss: 0.521, Train Acc: 0.751, Test Loss: 0.559, Test Acc: 0.744\n",
      "Epoch: 010, Train Loss: 0.507, Train Acc: 0.757, Test Loss: 0.582, Test Acc: 0.749\n",
      "Epoch: 011, Train Loss: 0.510, Train Acc: 0.762, Test Loss: 0.556, Test Acc: 0.753\n",
      "Epoch: 012, Train Loss: 0.503, Train Acc: 0.767, Test Loss: 0.548, Test Acc: 0.744\n",
      "Epoch: 013, Train Loss: 0.499, Train Acc: 0.758, Test Loss: 0.616, Test Acc: 0.740\n",
      "Epoch: 014, Train Loss: 0.507, Train Acc: 0.758, Test Loss: 0.579, Test Acc: 0.753\n",
      "Epoch: 015, Train Loss: 0.500, Train Acc: 0.769, Test Loss: 0.546, Test Acc: 0.749\n",
      "Epoch: 016, Train Loss: 0.500, Train Acc: 0.764, Test Loss: 0.559, Test Acc: 0.744\n",
      "Epoch: 017, Train Loss: 0.507, Train Acc: 0.766, Test Loss: 0.559, Test Acc: 0.740\n",
      "Epoch: 018, Train Loss: 0.505, Train Acc: 0.772, Test Loss: 0.581, Test Acc: 0.735\n",
      "Epoch: 019, Train Loss: 0.498, Train Acc: 0.763, Test Loss: 0.551, Test Acc: 0.744\n",
      "Epoch: 020, Train Loss: 0.498, Train Acc: 0.760, Test Loss: 0.588, Test Acc: 0.744\n",
      "Epoch: 021, Train Loss: 0.509, Train Acc: 0.749, Test Loss: 0.669, Test Acc: 0.749\n",
      "Epoch: 022, Train Loss: 0.524, Train Acc: 0.757, Test Loss: 0.652, Test Acc: 0.740\n",
      "Epoch: 023, Train Loss: 0.517, Train Acc: 0.753, Test Loss: 0.592, Test Acc: 0.758\n",
      "Epoch: 024, Train Loss: 0.498, Train Acc: 0.774, Test Loss: 0.555, Test Acc: 0.753\n",
      "Epoch: 025, Train Loss: 0.496, Train Acc: 0.767, Test Loss: 0.555, Test Acc: 0.744\n",
      "Epoch: 026, Train Loss: 0.494, Train Acc: 0.773, Test Loss: 0.582, Test Acc: 0.749\n",
      "Epoch: 027, Train Loss: 0.494, Train Acc: 0.773, Test Loss: 0.566, Test Acc: 0.753\n",
      "Epoch: 028, Train Loss: 0.489, Train Acc: 0.763, Test Loss: 0.589, Test Acc: 0.758\n",
      "Epoch: 029, Train Loss: 0.493, Train Acc: 0.775, Test Loss: 0.567, Test Acc: 0.753\n",
      "Epoch: 030, Train Loss: 0.492, Train Acc: 0.773, Test Loss: 0.556, Test Acc: 0.735\n",
      "Epoch: 031, Train Loss: 0.492, Train Acc: 0.772, Test Loss: 0.579, Test Acc: 0.744\n",
      "Epoch: 032, Train Loss: 0.499, Train Acc: 0.773, Test Loss: 0.552, Test Acc: 0.740\n",
      "Epoch: 033, Train Loss: 0.494, Train Acc: 0.764, Test Loss: 0.597, Test Acc: 0.753\n",
      "Epoch: 034, Train Loss: 0.488, Train Acc: 0.771, Test Loss: 0.561, Test Acc: 0.740\n",
      "Epoch: 035, Train Loss: 0.492, Train Acc: 0.774, Test Loss: 0.556, Test Acc: 0.753\n",
      "Epoch: 036, Train Loss: 0.500, Train Acc: 0.769, Test Loss: 0.571, Test Acc: 0.758\n",
      "Epoch: 037, Train Loss: 0.489, Train Acc: 0.764, Test Loss: 0.602, Test Acc: 0.753\n",
      "Epoch: 038, Train Loss: 0.489, Train Acc: 0.780, Test Loss: 0.567, Test Acc: 0.753\n",
      "Epoch: 039, Train Loss: 0.492, Train Acc: 0.763, Test Loss: 0.584, Test Acc: 0.762\n",
      "Epoch: 040, Train Loss: 0.503, Train Acc: 0.762, Test Loss: 0.607, Test Acc: 0.740\n",
      "Epoch: 041, Train Loss: 0.494, Train Acc: 0.767, Test Loss: 0.570, Test Acc: 0.753\n",
      "Epoch: 042, Train Loss: 0.493, Train Acc: 0.776, Test Loss: 0.553, Test Acc: 0.744\n",
      "Epoch: 043, Train Loss: 0.487, Train Acc: 0.776, Test Loss: 0.567, Test Acc: 0.753\n",
      "Epoch: 044, Train Loss: 0.485, Train Acc: 0.772, Test Loss: 0.578, Test Acc: 0.758\n",
      "Epoch: 045, Train Loss: 0.489, Train Acc: 0.762, Test Loss: 0.593, Test Acc: 0.762\n",
      "Epoch: 046, Train Loss: 0.499, Train Acc: 0.771, Test Loss: 0.531, Test Acc: 0.753\n",
      "Epoch: 047, Train Loss: 0.492, Train Acc: 0.779, Test Loss: 0.562, Test Acc: 0.749\n",
      "Epoch: 048, Train Loss: 0.483, Train Acc: 0.761, Test Loss: 0.594, Test Acc: 0.762\n",
      "Epoch: 049, Train Loss: 0.484, Train Acc: 0.775, Test Loss: 0.560, Test Acc: 0.744\n",
      "RUN:6,Test Acc:0.7444\n",
      "Epoch: 001, Train Loss: 0.611, Train Acc: 0.729, Test Loss: 0.611, Test Acc: 0.735\n",
      "Epoch: 002, Train Loss: 0.556, Train Acc: 0.740, Test Loss: 0.495, Test Acc: 0.735\n",
      "Epoch: 003, Train Loss: 0.539, Train Acc: 0.747, Test Loss: 0.572, Test Acc: 0.753\n",
      "Epoch: 004, Train Loss: 0.527, Train Acc: 0.745, Test Loss: 0.564, Test Acc: 0.726\n",
      "Epoch: 005, Train Loss: 0.517, Train Acc: 0.760, Test Loss: 0.558, Test Acc: 0.776\n",
      "Epoch: 006, Train Loss: 0.512, Train Acc: 0.756, Test Loss: 0.531, Test Acc: 0.749\n",
      "Epoch: 007, Train Loss: 0.508, Train Acc: 0.756, Test Loss: 0.603, Test Acc: 0.762\n",
      "Epoch: 008, Train Loss: 0.512, Train Acc: 0.749, Test Loss: 0.524, Test Acc: 0.740\n",
      "Epoch: 009, Train Loss: 0.505, Train Acc: 0.758, Test Loss: 0.561, Test Acc: 0.762\n",
      "Epoch: 010, Train Loss: 0.499, Train Acc: 0.764, Test Loss: 0.576, Test Acc: 0.744\n",
      "Epoch: 011, Train Loss: 0.499, Train Acc: 0.746, Test Loss: 0.595, Test Acc: 0.735\n",
      "Epoch: 012, Train Loss: 0.510, Train Acc: 0.751, Test Loss: 0.619, Test Acc: 0.726\n",
      "Epoch: 013, Train Loss: 0.493, Train Acc: 0.755, Test Loss: 0.573, Test Acc: 0.744\n",
      "Epoch: 014, Train Loss: 0.496, Train Acc: 0.765, Test Loss: 0.566, Test Acc: 0.740\n",
      "Epoch: 015, Train Loss: 0.494, Train Acc: 0.764, Test Loss: 0.576, Test Acc: 0.762\n",
      "Epoch: 016, Train Loss: 0.493, Train Acc: 0.769, Test Loss: 0.589, Test Acc: 0.753\n",
      "Epoch: 017, Train Loss: 0.496, Train Acc: 0.758, Test Loss: 0.583, Test Acc: 0.749\n",
      "Epoch: 018, Train Loss: 0.497, Train Acc: 0.755, Test Loss: 0.537, Test Acc: 0.753\n",
      "Epoch: 019, Train Loss: 0.515, Train Acc: 0.755, Test Loss: 0.545, Test Acc: 0.740\n",
      "Epoch: 020, Train Loss: 0.505, Train Acc: 0.760, Test Loss: 0.551, Test Acc: 0.758\n",
      "Epoch: 021, Train Loss: 0.497, Train Acc: 0.765, Test Loss: 0.538, Test Acc: 0.758\n",
      "Epoch: 022, Train Loss: 0.495, Train Acc: 0.764, Test Loss: 0.560, Test Acc: 0.749\n",
      "Epoch: 023, Train Loss: 0.490, Train Acc: 0.765, Test Loss: 0.561, Test Acc: 0.740\n",
      "Epoch: 024, Train Loss: 0.496, Train Acc: 0.762, Test Loss: 0.534, Test Acc: 0.731\n",
      "Epoch: 025, Train Loss: 0.496, Train Acc: 0.770, Test Loss: 0.528, Test Acc: 0.762\n",
      "Epoch: 026, Train Loss: 0.498, Train Acc: 0.767, Test Loss: 0.552, Test Acc: 0.744\n",
      "Epoch: 027, Train Loss: 0.490, Train Acc: 0.769, Test Loss: 0.558, Test Acc: 0.767\n",
      "Epoch: 028, Train Loss: 0.490, Train Acc: 0.772, Test Loss: 0.557, Test Acc: 0.776\n",
      "Epoch: 029, Train Loss: 0.493, Train Acc: 0.764, Test Loss: 0.548, Test Acc: 0.749\n",
      "Epoch: 030, Train Loss: 0.490, Train Acc: 0.762, Test Loss: 0.557, Test Acc: 0.762\n",
      "Epoch: 031, Train Loss: 0.489, Train Acc: 0.771, Test Loss: 0.537, Test Acc: 0.753\n",
      "Epoch: 032, Train Loss: 0.489, Train Acc: 0.771, Test Loss: 0.552, Test Acc: 0.758\n",
      "Epoch: 033, Train Loss: 0.486, Train Acc: 0.763, Test Loss: 0.556, Test Acc: 0.749\n",
      "Epoch: 034, Train Loss: 0.500, Train Acc: 0.772, Test Loss: 0.554, Test Acc: 0.749\n",
      "Epoch: 035, Train Loss: 0.492, Train Acc: 0.767, Test Loss: 0.584, Test Acc: 0.753\n",
      "Epoch: 036, Train Loss: 0.493, Train Acc: 0.761, Test Loss: 0.570, Test Acc: 0.722\n",
      "Epoch: 037, Train Loss: 0.495, Train Acc: 0.769, Test Loss: 0.542, Test Acc: 0.758\n",
      "Epoch: 038, Train Loss: 0.485, Train Acc: 0.770, Test Loss: 0.543, Test Acc: 0.767\n",
      "Epoch: 039, Train Loss: 0.487, Train Acc: 0.773, Test Loss: 0.537, Test Acc: 0.753\n",
      "Epoch: 040, Train Loss: 0.485, Train Acc: 0.760, Test Loss: 0.546, Test Acc: 0.735\n",
      "Epoch: 041, Train Loss: 0.486, Train Acc: 0.775, Test Loss: 0.539, Test Acc: 0.744\n",
      "Epoch: 042, Train Loss: 0.487, Train Acc: 0.772, Test Loss: 0.597, Test Acc: 0.753\n",
      "Epoch: 043, Train Loss: 0.482, Train Acc: 0.766, Test Loss: 0.524, Test Acc: 0.758\n",
      "Epoch: 044, Train Loss: 0.485, Train Acc: 0.774, Test Loss: 0.589, Test Acc: 0.771\n",
      "Epoch: 045, Train Loss: 0.488, Train Acc: 0.771, Test Loss: 0.535, Test Acc: 0.740\n",
      "Epoch: 046, Train Loss: 0.484, Train Acc: 0.776, Test Loss: 0.550, Test Acc: 0.749\n",
      "Epoch: 047, Train Loss: 0.484, Train Acc: 0.774, Test Loss: 0.531, Test Acc: 0.758\n",
      "Epoch: 048, Train Loss: 0.480, Train Acc: 0.778, Test Loss: 0.532, Test Acc: 0.762\n",
      "Epoch: 049, Train Loss: 0.483, Train Acc: 0.766, Test Loss: 0.528, Test Acc: 0.740\n",
      "RUN:7,Test Acc:0.7399\n",
      "Epoch: 001, Train Loss: 0.620, Train Acc: 0.742, Test Loss: 0.661, Test Acc: 0.726\n",
      "Epoch: 002, Train Loss: 0.548, Train Acc: 0.739, Test Loss: 0.659, Test Acc: 0.726\n",
      "Epoch: 003, Train Loss: 0.525, Train Acc: 0.747, Test Loss: 0.673, Test Acc: 0.735\n",
      "Epoch: 004, Train Loss: 0.522, Train Acc: 0.749, Test Loss: 0.664, Test Acc: 0.735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Loss: 0.521, Train Acc: 0.751, Test Loss: 0.675, Test Acc: 0.744\n",
      "Epoch: 006, Train Loss: 0.519, Train Acc: 0.746, Test Loss: 0.699, Test Acc: 0.735\n",
      "Epoch: 007, Train Loss: 0.508, Train Acc: 0.760, Test Loss: 0.687, Test Acc: 0.740\n",
      "Epoch: 008, Train Loss: 0.503, Train Acc: 0.763, Test Loss: 0.683, Test Acc: 0.749\n",
      "Epoch: 009, Train Loss: 0.504, Train Acc: 0.761, Test Loss: 0.683, Test Acc: 0.749\n",
      "Epoch: 010, Train Loss: 0.500, Train Acc: 0.764, Test Loss: 0.696, Test Acc: 0.735\n",
      "Epoch: 011, Train Loss: 0.506, Train Acc: 0.749, Test Loss: 0.702, Test Acc: 0.740\n",
      "Epoch: 012, Train Loss: 0.500, Train Acc: 0.769, Test Loss: 0.681, Test Acc: 0.744\n",
      "Epoch: 013, Train Loss: 0.496, Train Acc: 0.765, Test Loss: 0.694, Test Acc: 0.749\n",
      "Epoch: 014, Train Loss: 0.494, Train Acc: 0.761, Test Loss: 0.699, Test Acc: 0.753\n",
      "Epoch: 015, Train Loss: 0.493, Train Acc: 0.769, Test Loss: 0.694, Test Acc: 0.749\n",
      "Epoch: 016, Train Loss: 0.490, Train Acc: 0.765, Test Loss: 0.691, Test Acc: 0.749\n",
      "Epoch: 017, Train Loss: 0.495, Train Acc: 0.764, Test Loss: 0.681, Test Acc: 0.740\n",
      "Epoch: 018, Train Loss: 0.491, Train Acc: 0.771, Test Loss: 0.701, Test Acc: 0.758\n",
      "Epoch: 019, Train Loss: 0.489, Train Acc: 0.767, Test Loss: 0.680, Test Acc: 0.749\n",
      "Epoch: 020, Train Loss: 0.488, Train Acc: 0.769, Test Loss: 0.730, Test Acc: 0.749\n",
      "Epoch: 021, Train Loss: 0.485, Train Acc: 0.775, Test Loss: 0.695, Test Acc: 0.749\n",
      "Epoch: 022, Train Loss: 0.494, Train Acc: 0.775, Test Loss: 0.706, Test Acc: 0.758\n",
      "Epoch: 023, Train Loss: 0.485, Train Acc: 0.778, Test Loss: 0.701, Test Acc: 0.749\n",
      "Epoch: 024, Train Loss: 0.483, Train Acc: 0.756, Test Loss: 0.703, Test Acc: 0.749\n",
      "Epoch: 025, Train Loss: 0.493, Train Acc: 0.772, Test Loss: 0.707, Test Acc: 0.758\n",
      "Epoch: 026, Train Loss: 0.486, Train Acc: 0.762, Test Loss: 0.757, Test Acc: 0.762\n",
      "Epoch: 027, Train Loss: 0.486, Train Acc: 0.771, Test Loss: 0.699, Test Acc: 0.749\n",
      "Epoch: 028, Train Loss: 0.482, Train Acc: 0.779, Test Loss: 0.716, Test Acc: 0.749\n",
      "Epoch: 029, Train Loss: 0.483, Train Acc: 0.779, Test Loss: 0.723, Test Acc: 0.762\n",
      "Epoch: 030, Train Loss: 0.480, Train Acc: 0.778, Test Loss: 0.723, Test Acc: 0.749\n",
      "Epoch: 031, Train Loss: 0.479, Train Acc: 0.778, Test Loss: 0.704, Test Acc: 0.749\n",
      "Epoch: 032, Train Loss: 0.478, Train Acc: 0.772, Test Loss: 0.702, Test Acc: 0.744\n",
      "Epoch: 033, Train Loss: 0.496, Train Acc: 0.763, Test Loss: 0.701, Test Acc: 0.744\n",
      "Epoch: 034, Train Loss: 0.477, Train Acc: 0.770, Test Loss: 0.755, Test Acc: 0.758\n",
      "Epoch: 035, Train Loss: 0.477, Train Acc: 0.763, Test Loss: 0.702, Test Acc: 0.753\n",
      "Epoch: 036, Train Loss: 0.481, Train Acc: 0.772, Test Loss: 0.728, Test Acc: 0.753\n",
      "Epoch: 037, Train Loss: 0.477, Train Acc: 0.779, Test Loss: 0.739, Test Acc: 0.758\n",
      "Epoch: 038, Train Loss: 0.479, Train Acc: 0.785, Test Loss: 0.721, Test Acc: 0.749\n",
      "Epoch: 039, Train Loss: 0.472, Train Acc: 0.769, Test Loss: 0.705, Test Acc: 0.753\n",
      "Epoch: 040, Train Loss: 0.480, Train Acc: 0.785, Test Loss: 0.739, Test Acc: 0.758\n",
      "Epoch: 041, Train Loss: 0.477, Train Acc: 0.782, Test Loss: 0.746, Test Acc: 0.762\n",
      "Epoch: 042, Train Loss: 0.475, Train Acc: 0.778, Test Loss: 0.731, Test Acc: 0.749\n",
      "Epoch: 043, Train Loss: 0.473, Train Acc: 0.779, Test Loss: 0.719, Test Acc: 0.744\n",
      "Epoch: 044, Train Loss: 0.482, Train Acc: 0.773, Test Loss: 0.721, Test Acc: 0.753\n",
      "Epoch: 045, Train Loss: 0.481, Train Acc: 0.784, Test Loss: 0.735, Test Acc: 0.758\n",
      "Epoch: 046, Train Loss: 0.471, Train Acc: 0.779, Test Loss: 0.722, Test Acc: 0.749\n",
      "Epoch: 047, Train Loss: 0.475, Train Acc: 0.775, Test Loss: 0.747, Test Acc: 0.749\n",
      "Epoch: 048, Train Loss: 0.474, Train Acc: 0.778, Test Loss: 0.764, Test Acc: 0.762\n",
      "Epoch: 049, Train Loss: 0.473, Train Acc: 0.765, Test Loss: 0.705, Test Acc: 0.735\n",
      "RUN:8,Test Acc:0.7354\n",
      "Epoch: 001, Train Loss: 0.612, Train Acc: 0.744, Test Loss: 0.547, Test Acc: 0.767\n",
      "Epoch: 002, Train Loss: 0.552, Train Acc: 0.730, Test Loss: 0.569, Test Acc: 0.758\n",
      "Epoch: 003, Train Loss: 0.531, Train Acc: 0.744, Test Loss: 0.550, Test Acc: 0.762\n",
      "Epoch: 004, Train Loss: 0.525, Train Acc: 0.744, Test Loss: 0.553, Test Acc: 0.762\n",
      "Epoch: 005, Train Loss: 0.521, Train Acc: 0.744, Test Loss: 0.565, Test Acc: 0.749\n",
      "Epoch: 006, Train Loss: 0.519, Train Acc: 0.739, Test Loss: 0.563, Test Acc: 0.776\n",
      "Epoch: 007, Train Loss: 0.519, Train Acc: 0.742, Test Loss: 0.590, Test Acc: 0.753\n",
      "Epoch: 008, Train Loss: 0.516, Train Acc: 0.752, Test Loss: 0.561, Test Acc: 0.762\n",
      "Epoch: 009, Train Loss: 0.512, Train Acc: 0.756, Test Loss: 0.579, Test Acc: 0.749\n",
      "Epoch: 010, Train Loss: 0.504, Train Acc: 0.757, Test Loss: 0.573, Test Acc: 0.753\n",
      "Epoch: 011, Train Loss: 0.511, Train Acc: 0.752, Test Loss: 0.585, Test Acc: 0.758\n",
      "Epoch: 012, Train Loss: 0.510, Train Acc: 0.745, Test Loss: 0.588, Test Acc: 0.749\n",
      "Epoch: 013, Train Loss: 0.505, Train Acc: 0.760, Test Loss: 0.584, Test Acc: 0.771\n",
      "Epoch: 014, Train Loss: 0.506, Train Acc: 0.753, Test Loss: 0.587, Test Acc: 0.762\n",
      "Epoch: 015, Train Loss: 0.505, Train Acc: 0.758, Test Loss: 0.579, Test Acc: 0.776\n",
      "Epoch: 016, Train Loss: 0.501, Train Acc: 0.751, Test Loss: 0.601, Test Acc: 0.762\n",
      "Epoch: 017, Train Loss: 0.508, Train Acc: 0.758, Test Loss: 0.585, Test Acc: 0.776\n",
      "Epoch: 018, Train Loss: 0.504, Train Acc: 0.753, Test Loss: 0.594, Test Acc: 0.753\n",
      "Epoch: 019, Train Loss: 0.497, Train Acc: 0.758, Test Loss: 0.582, Test Acc: 0.780\n",
      "Epoch: 020, Train Loss: 0.500, Train Acc: 0.748, Test Loss: 0.597, Test Acc: 0.762\n",
      "Epoch: 021, Train Loss: 0.497, Train Acc: 0.758, Test Loss: 0.589, Test Acc: 0.767\n",
      "Epoch: 022, Train Loss: 0.498, Train Acc: 0.758, Test Loss: 0.606, Test Acc: 0.758\n",
      "Epoch: 023, Train Loss: 0.499, Train Acc: 0.756, Test Loss: 0.602, Test Acc: 0.762\n",
      "Epoch: 024, Train Loss: 0.511, Train Acc: 0.764, Test Loss: 0.584, Test Acc: 0.753\n",
      "Epoch: 025, Train Loss: 0.498, Train Acc: 0.758, Test Loss: 0.602, Test Acc: 0.771\n",
      "Epoch: 026, Train Loss: 0.505, Train Acc: 0.730, Test Loss: 0.644, Test Acc: 0.735\n",
      "Epoch: 027, Train Loss: 0.531, Train Acc: 0.754, Test Loss: 0.612, Test Acc: 0.771\n",
      "Epoch: 028, Train Loss: 0.530, Train Acc: 0.744, Test Loss: 0.621, Test Acc: 0.749\n",
      "Epoch: 029, Train Loss: 0.508, Train Acc: 0.758, Test Loss: 0.592, Test Acc: 0.762\n",
      "Epoch: 030, Train Loss: 0.494, Train Acc: 0.757, Test Loss: 0.599, Test Acc: 0.753\n",
      "Epoch: 031, Train Loss: 0.498, Train Acc: 0.756, Test Loss: 0.616, Test Acc: 0.753\n",
      "Epoch: 032, Train Loss: 0.500, Train Acc: 0.760, Test Loss: 0.591, Test Acc: 0.758\n",
      "Epoch: 033, Train Loss: 0.494, Train Acc: 0.748, Test Loss: 0.622, Test Acc: 0.749\n",
      "Epoch: 034, Train Loss: 0.500, Train Acc: 0.765, Test Loss: 0.591, Test Acc: 0.758\n",
      "Epoch: 035, Train Loss: 0.496, Train Acc: 0.766, Test Loss: 0.598, Test Acc: 0.758\n",
      "Epoch: 036, Train Loss: 0.493, Train Acc: 0.763, Test Loss: 0.591, Test Acc: 0.758\n",
      "Epoch: 037, Train Loss: 0.489, Train Acc: 0.758, Test Loss: 0.594, Test Acc: 0.762\n",
      "Epoch: 038, Train Loss: 0.492, Train Acc: 0.757, Test Loss: 0.599, Test Acc: 0.758\n",
      "Epoch: 039, Train Loss: 0.490, Train Acc: 0.765, Test Loss: 0.598, Test Acc: 0.744\n",
      "Epoch: 040, Train Loss: 0.489, Train Acc: 0.746, Test Loss: 0.628, Test Acc: 0.749\n",
      "Epoch: 041, Train Loss: 0.497, Train Acc: 0.761, Test Loss: 0.597, Test Acc: 0.762\n",
      "Epoch: 042, Train Loss: 0.493, Train Acc: 0.760, Test Loss: 0.603, Test Acc: 0.762\n",
      "Epoch: 043, Train Loss: 0.491, Train Acc: 0.762, Test Loss: 0.592, Test Acc: 0.749\n",
      "Epoch: 044, Train Loss: 0.489, Train Acc: 0.766, Test Loss: 0.601, Test Acc: 0.780\n",
      "Epoch: 045, Train Loss: 0.491, Train Acc: 0.766, Test Loss: 0.595, Test Acc: 0.749\n",
      "Epoch: 046, Train Loss: 0.490, Train Acc: 0.769, Test Loss: 0.600, Test Acc: 0.744\n",
      "Epoch: 047, Train Loss: 0.493, Train Acc: 0.769, Test Loss: 0.596, Test Acc: 0.753\n",
      "Epoch: 048, Train Loss: 0.486, Train Acc: 0.766, Test Loss: 0.598, Test Acc: 0.758\n",
      "Epoch: 049, Train Loss: 0.492, Train Acc: 0.769, Test Loss: 0.597, Test Acc: 0.753\n",
      "RUN:9,Test Acc:0.7534\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    k = 0\n",
    "    nG = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        #print(data.x.size(), data.edge_index.size(),data.batch.size(), k)\n",
    "        data\n",
    "        nG += data.num_graphs\n",
    "        optimizer.zero_grad()\n",
    "        out= model(data.x, data.edge_index, data.batch) # data.batch  torch.Size([783])\n",
    "        loss = F.nll_loss(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * loss.item()\n",
    "        optimizer.step()\n",
    "        k = k + 1\n",
    "    #print(\"Training graphs per epoch\", nG)\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(pred, data.y.view(-1))\n",
    "        correct += pred.max(dim=1)[1].eq(data.y.view(-1)).sum().item()\n",
    "\n",
    "    return loss, correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = start_patience = 50\n",
    "for i in range(10):\n",
    "    dataset = dataset.shuffle()\n",
    "    train_dataset = dataset[:890]\n",
    "    test_dataset = dataset[890:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    for epoch in range(1, 50):\n",
    "        train_loss = train(epoch)\n",
    "        _, train_acc = test(train_loader)\n",
    "        test_loss, test_acc = test(test_loader)\n",
    "        print('Epoch: {:03d}, '\n",
    "              'Train Loss: {:.3f}, Train Acc: {:.3f}, '\n",
    "              'Test Loss: {:.3f}, Test Acc: {:.3f}'.format(epoch, train_loss,\n",
    "                                                           train_acc,\n",
    "                                                            test_loss,\n",
    "                                                           test_acc))\n",
    "    print(f'RUN:{i},Test Acc:{test_acc:.4f}')\n",
    "    resultados.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28d17cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7878)\n",
      "tensor(0.0451)\n"
     ]
    }
   ],
   "source": [
    "res= torch.Tensor(resultados)\n",
    "print(torch.mean(res))\n",
    "print(torch.std(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "addaf0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 52], x=[16, 3], y=[1], batch=[16], ptr=[2])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset[2:], batch_size=1, shuffle=False)\n",
    "sample = next(iter(test_loader))\n",
    "#sample.batch =torch.tensor(1)\n",
    "print(sample)\n",
    "print(sample.y)\n",
    "out = model(sample.x,sample.edge_index, sample.batch)  # Perform a single forward pass.\n",
    "pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "print(pred)\n",
    "correct = 0\n",
    "correct += int((pred[0] == sample[0].y).sum())  # Check against ground-truth labels.\n",
    "print(\"Accuracy: \",(correct/1)*100)#Batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da05ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import *\n",
    "#loader =  DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "grafo_init = sample\n",
    "print(grafo_init)\n",
    "A = to_networkx(grafo_init, to_undirected=True)\n",
    "l=[]\n",
    "for a in A.nodes:\n",
    "    l.append(a)\n",
    "ed= []\n",
    "for e in A.edges:\n",
    "    ed.append(e)\n",
    "import igraph as ig\n",
    "import chart_studio.plotly\n",
    "Edges= ed\n",
    "G=ig.Graph(Edges, directed=False)\n",
    "labels= l\n",
    "#groups = A.node_attr_dict_factory\n",
    "N = len(A.nodes)\n",
    "print(N)\n",
    "layt=G.layout('kk', dim=3)\n",
    "print(layt)\n",
    "Xn=[layt[k][0] for k in range(N)]# x-coordinates of nodes\n",
    "Yn=[layt[k][1] for k in range(N)]# y-coordinates\n",
    "Zn=[layt[k][2] for k in range(N)]# z-coordinates\n",
    "Xe=[]\n",
    "Ye=[]\n",
    "Ze=[]\n",
    "print(Edges)\n",
    "for e in Edges:\n",
    "    Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends\n",
    "    Ye+=[layt[e[0]][1],layt[e[1]][1], None]\n",
    "    Ze+=[layt[e[0]][2],layt[e[1]][2], None]\n",
    "nx.draw(A)\n",
    "node_adjacencies = []\n",
    "node_text = []\n",
    "for node, adjacencies in enumerate(A.nodes()):\n",
    "    #print(adjacencies,critical_nodes)\n",
    "    if adjacencies in critical_nodes :\n",
    "        node_adjacencies.append('rgb(256,0,0)')\n",
    "    else:\n",
    "        node_adjacencies.append('rgb(0,0,0)')\n",
    "    #node_text.append('# of connections: '+str(len(adjacencies[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2570a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chart_studio import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1=go.Scatter3d(x=Xe,\n",
    "               y=Ye,\n",
    "               z=Ze,\n",
    "               mode='lines',\n",
    "               line=dict(color='rgb(125,125,125)', width=1),\n",
    "               hoverinfo='none'\n",
    "               )\n",
    "\n",
    "trace2=go.Scatter3d(x=Xn,\n",
    "               y=Yn,\n",
    "               z=Zn,\n",
    "               mode='markers',\n",
    "               name='actors',\n",
    "               marker=dict(symbol='circle',\n",
    "                             size=6,\n",
    "                             #color='#ff7f0e',\n",
    "                             colorscale='Viridis',\n",
    "                             line=dict(color='rgb(50,50,50)', width=0.5)\n",
    "                             ),\n",
    "               text=labels,\n",
    "               hoverinfo='text'\n",
    "               )\n",
    "trace2.marker.color = node_adjacencies\n",
    "#print(trace2)\n",
    "axis=dict(showbackground=False,\n",
    "          showline=False,\n",
    "          zeroline=False,\n",
    "          showgrid=False,\n",
    "          showticklabels=False,\n",
    "          title=''\n",
    "          )\n",
    "\n",
    "layout = go.Layout(\n",
    "         width=1000,\n",
    "         height=1000,\n",
    "         showlegend=False,\n",
    "         scene=dict(\n",
    "             xaxis=dict(axis),\n",
    "             yaxis=dict(axis),\n",
    "             zaxis=dict(axis),\n",
    "        ),\n",
    "     margin=dict(\n",
    "        t=100\n",
    "    ),\n",
    "    hovermode='closest',    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import plotly.graph_objs as go\n",
    "from  plotly.offline import plot\n",
    "import chart_studio.plotly as py\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\n",
    "init_notebook_mode(connected='true')\n",
    "data=[trace1, trace2]\n",
    "fig=go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='Les-Miserables')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
